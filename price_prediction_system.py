"""
AMDä¼˜åŒ–LSTMä»·æ ¼é¢„æµ‹ç³»ç»Ÿ
ä¸“é—¨é’ˆå¯¹AMD CPUå’ŒGPUè®¾è®¡ï¼Œå……åˆ†åˆ©ç”¨å…¨éƒ¨2000æ¡æ•°æ®
æ”¯æŒROCmå’Œé«˜æ•ˆCPUå¹¶è¡Œè®¡ç®—
"""
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from datetime import datetime, timedelta
import warnings
import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import time
import os
from multi_source_fetcher import MultiSourceDataFetcher

warnings.filterwarnings('ignore')

# AMD GPUä¼˜åŒ–é…ç½®
def setup_amd_device():
    """è®¾ç½®AMD GPUè®¾å¤‡é…ç½®"""
    device = "cpu"  # é»˜è®¤CPU
    device_name = "AMD CPU"
    
    if torch.cuda.is_available():
        device = "cuda"
        device_name = torch.cuda.get_device_name(0)
        print(f"ğŸš€ æ£€æµ‹åˆ°GPU: {device_name}")
        
        # é’ˆå¯¹AMD GPUçš„ä¼˜åŒ–è®¾ç½®
        if "MI" in device_name or "Instinct" in device_name or "Radeon" in device_name:
            print("ğŸ”§ åº”ç”¨AMD GPUä¼˜åŒ–è®¾ç½®...")
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.backends.cudnn.enabled = True
            
        # è®¾ç½®æ˜¾å­˜ç®¡ç†
        torch.cuda.empty_cache()
    else:
        print("ğŸ”§ ä½¿ç”¨AMD CPUä¼˜åŒ–è®¾ç½®...")
        # AMD CPUä¼˜åŒ–
        torch.set_num_threads(os.cpu_count())  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        
    return device, device_name

class LSTMPricePredictor(nn.Module):
    """
    ä¼˜åŒ–çš„LSTMä»·æ ¼é¢„æµ‹æ¨¡å‹
    é’ˆå¯¹AMDç¡¬ä»¶ä¼˜åŒ–ï¼Œæ”¯æŒå¤šå±‚LSTMå’ŒDropout
    """
    def __init__(self, input_size=5, hidden_size=128, num_layers=3, dropout=0.2, output_size=4):
        super(LSTMPricePredictor, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.input_size = input_size
        
        # å¤šå±‚LSTM
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
            bidirectional=False
        )
        
        # Dropoutå±‚
        self.dropout = nn.Dropout(dropout)
        
        # å…¨è¿æ¥å±‚é¢„æµ‹OHLC
        self.fc_layers = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 4, output_size)  # OHLC
        )
        
        # æˆäº¤é‡é¢„æµ‹å±‚
        self.volume_layer = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 4, 1),
            nn.ReLU()  # æˆäº¤é‡å¿…é¡»ä¸ºæ­£
        )
        
    def forward(self, x):
        batch_size = x.size(0)
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)
        
        # LSTMå‰å‘ä¼ æ’­
        lstm_out, _ = self.lstm(x, (h0, c0))
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]
        last_output = self.dropout(last_output)
        
        # é¢„æµ‹OHLCä»·æ ¼
        ohlc_pred = self.fc_layers(last_output)
        
        # é¢„æµ‹æˆäº¤é‡
        volume_pred = self.volume_layer(last_output)
        
        return ohlc_pred, volume_pred

class AMDLSTMPredictionSystem:
    """AMDä¼˜åŒ–çš„LSTMä»·æ ¼é¢„æµ‹ç³»ç»Ÿ"""
    
    def __init__(self, symbol="DOGEUSDT", sequence_length=30):
        self.symbol = symbol
        self.sequence_length = sequence_length
        self.device, self.device_name = setup_amd_device()
        
        # æ•°æ®æ ‡å‡†åŒ–å™¨
        self.price_scaler = MinMaxScaler()
        self.volume_scaler = MinMaxScaler()
        
        # æ¨¡å‹
        self.model = None
        self.trained = False
        
        print(f"ğŸ“± è¿è¡Œè®¾å¤‡: {self.device_name}")
        print(f"ğŸ”¢ åºåˆ—é•¿åº¦: {sequence_length}")
        
    def prepare_data(self, data, train_ratio=0.85):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œå……åˆ†åˆ©ç”¨å…¨éƒ¨2000æ¡æ•°æ®
        """
        print(f"ğŸ“Š å‡†å¤‡æ•°æ®: {len(data)} æ¡è®°å½•")
        
        # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´æ’åº
        data = data.sort_index()
        
        # æå–OHLCVæ•°æ®
        ohlc_data = data[['open', 'high', 'low', 'close']].values
        volume_data = data[['volume']].values
        
        # æ•°æ®æ ‡å‡†åŒ–
        ohlc_scaled = self.price_scaler.fit_transform(ohlc_data)
        volume_scaled = self.volume_scaler.fit_transform(volume_data)
        
        # åˆå¹¶ç‰¹å¾
        combined_features = np.concatenate([ohlc_scaled, volume_scaled], axis=1)
        
        # åˆ›å»ºåºåˆ—æ•°æ®
        sequences = []
        targets_ohlc = []
        targets_volume = []
        
        for i in range(self.sequence_length, len(combined_features)):
            # è¾“å…¥åºåˆ—
            seq = combined_features[i-self.sequence_length:i]
            sequences.append(seq)
            
            # ç›®æ ‡å€¼ï¼šä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„OHLCå’Œæˆäº¤é‡
            targets_ohlc.append(ohlc_scaled[i])
            targets_volume.append(volume_scaled[i])
        
        sequences = np.array(sequences)
        targets_ohlc = np.array(targets_ohlc)
        targets_volume = np.array(targets_volume)
        
        print(f"âœ… ç”Ÿæˆåºåˆ—: {len(sequences)} ä¸ª")
        print(f"ğŸ“ åºåˆ—å½¢çŠ¶: {sequences.shape}")
        
        # åˆ†å‰²è®­ç»ƒæµ‹è¯•é›†ï¼Œå……åˆ†åˆ©ç”¨æ•°æ®
        split_idx = int(len(sequences) * train_ratio)
        
        X_train = sequences[:split_idx]
        X_test = sequences[split_idx:]
        y_train_ohlc = targets_ohlc[:split_idx]
        y_test_ohlc = targets_ohlc[split_idx:]
        y_train_vol = targets_volume[:split_idx]
        y_test_vol = targets_volume[split_idx:]
        
        print(f"ğŸ‹ï¸ è®­ç»ƒé›†: {len(X_train)} ä¸ªåºåˆ—")
        print(f"ğŸ§ª æµ‹è¯•é›†: {len(X_test)} ä¸ªåºåˆ—")
        print(f"ğŸ“ˆ æ•°æ®åˆ©ç”¨ç‡: {len(sequences)/len(data)*100:.1f}%")
        
        return (X_train, X_test, y_train_ohlc, y_test_ohlc, 
                y_train_vol, y_test_vol, data.index[self.sequence_length:])
    
    def create_model(self, input_size=5):
        """åˆ›å»ºLSTMæ¨¡å‹"""
        model = LSTMPricePredictor(
            input_size=input_size,
            hidden_size=128,  # AMD GPUä¼˜åŒ–çš„éšè—å±‚å¤§å°
            num_layers=3,     # æ·±åº¦LSTM
            dropout=0.2,
            output_size=4     # OHLC
        )
        return model.to(self.device)
    
    def train_model(self, X_train, y_train_ohlc, y_train_vol, X_test, y_test_ohlc, y_test_vol, 
                   batch_size=32, epochs=100, learning_rate=0.001):
        """
        è®­ç»ƒLSTMæ¨¡å‹ï¼Œä½¿ç”¨AMDä¼˜åŒ–çš„è®¾ç½®
        """
        print("ğŸš€ å¼€å§‹è®­ç»ƒLSTMæ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = self.create_model(input_size=X_train.shape[2])
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_ohlc_tensor = torch.FloatTensor(y_train_ohlc).to(self.device)
        y_train_vol_tensor = torch.FloatTensor(y_train_vol).to(self.device)
        
        X_test_tensor = torch.FloatTensor(X_test).to(self.device)
        y_test_ohlc_tensor = torch.FloatTensor(y_test_ohlc).to(self.device)
        y_test_vol_tensor = torch.FloatTensor(y_test_vol).to(self.device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_ohlc_tensor, y_train_vol_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                                num_workers=0, pin_memory=True if self.device == "cuda" else False)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion_price = nn.MSELoss()
        criterion_volume = nn.MSELoss()
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', 
                                                        factor=0.5, patience=10)
        
        # è®­ç»ƒå¾ªç¯
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        patience = 15
        patience_counter = 0
        
        print(f"ğŸ”§ è®¾å¤‡: {self.device_name}")
        print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"ğŸ”„ è®¡åˆ’è®­ç»ƒè½®æ•°: {epochs}")
        
        for epoch in range(epochs):
            # è®­ç»ƒæ¨¡å¼
            self.model.train()
            epoch_train_loss = 0
            
            for batch_idx, (batch_x, batch_y_ohlc, batch_y_vol) in enumerate(train_loader):
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                ohlc_pred, vol_pred = self.model(batch_x)
                
                # è®¡ç®—æŸå¤±
                loss_ohlc = criterion_price(ohlc_pred, batch_y_ohlc)
                loss_vol = criterion_volume(vol_pred.squeeze(), batch_y_vol.squeeze())
                total_loss = loss_ohlc + 0.1 * loss_vol  # ä»·æ ¼æŸå¤±æƒé‡æ›´é«˜
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                epoch_train_loss += total_loss.item()
            
            # éªŒè¯
            self.model.eval()
            with torch.no_grad():
                ohlc_pred_val, vol_pred_val = self.model(X_test_tensor)
                val_loss_ohlc = criterion_price(ohlc_pred_val, y_test_ohlc_tensor)
                val_loss_vol = criterion_volume(vol_pred_val.squeeze(), y_test_vol_tensor.squeeze())
                val_loss = val_loss_ohlc + 0.1 * val_loss_vol
            
            avg_train_loss = epoch_train_loss / len(train_loader)
            train_losses.append(avg_train_loss)
            val_losses.append(val_loss.item())
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_loss)
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.model.state_dict(), f'best_lstm_model_{self.symbol}.pth')
            else:
                patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"è½®æ¬¡ [{epoch+1}/{epochs}] - "
                      f"è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}, "
                      f"éªŒè¯æŸå¤±: {val_loss:.6f}, "
                      f"å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œç¬¬ {epoch+1} è½®")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        self.model.load_state_dict(torch.load(f'best_lstm_model_{self.symbol}.pth'))
        self.trained = True
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        return train_losses, val_losses
    
    def predict_future(self, data, prediction_hours=24):
        """
        é¢„æµ‹æœªæ¥ä»·æ ¼
        """
        if not self.trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        print(f"ğŸ”® é¢„æµ‹æœªæ¥ {prediction_hours} å°æ—¶ä»·æ ¼...")
        
        self.model.eval()
        
        # å‡†å¤‡æœ€åä¸€ä¸ªåºåˆ—
        ohlc_data = data[['open', 'high', 'low', 'close']].values
        volume_data = data[['volume']].values
        
        ohlc_scaled = self.price_scaler.transform(ohlc_data)
        volume_scaled = self.volume_scaler.transform(volume_data)
        combined_features = np.concatenate([ohlc_scaled, volume_scaled], axis=1)
        
        # å–æœ€åä¸€ä¸ªåºåˆ—
        last_sequence = combined_features[-self.sequence_length:]
        current_sequence = torch.FloatTensor(last_sequence).unsqueeze(0).to(self.device)
        
        predictions = []
        
        with torch.no_grad():
            for _ in range(prediction_hours):
                # é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹
                ohlc_pred, vol_pred = self.model(current_sequence)
                
                # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
                ohlc_pred_unscaled = self.price_scaler.inverse_transform(ohlc_pred.cpu().numpy())
                vol_pred_unscaled = self.volume_scaler.inverse_transform(vol_pred.cpu().numpy())
                
                # ç¡®ä¿ä»·æ ¼é€»è¾‘æ€§
                open_price = ohlc_pred_unscaled[0][0]
                high_price = max(ohlc_pred_unscaled[0][1], open_price, ohlc_pred_unscaled[0][3])
                low_price = min(ohlc_pred_unscaled[0][2], open_price, ohlc_pred_unscaled[0][3])
                close_price = ohlc_pred_unscaled[0][3]
                volume = max(0, vol_pred_unscaled[0][0])
                
                predictions.append({
                    'open': open_price,
                    'high': high_price, 
                    'low': low_price,
                    'close': close_price,
                    'volume': volume
                })
                
                # æ›´æ–°åºåˆ—ç”¨äºä¸‹ä¸€æ¬¡é¢„æµ‹
                new_point = np.array([[open_price, high_price, low_price, close_price, volume]])
                new_point_scaled = np.concatenate([
                    self.price_scaler.transform(new_point[:, :4]),
                    self.volume_scaler.transform(new_point[:, 4:5])
                ], axis=1)
                
                # æ»‘åŠ¨çª—å£æ›´æ–°
                current_sequence = torch.cat([
                    current_sequence[:, 1:, :],
                    torch.FloatTensor(new_point_scaled).unsqueeze(0).to(self.device)
                ], dim=1)
        
        # ç”Ÿæˆæ—¶é—´ç´¢å¼•
        last_time = data.index[-1]
        future_times = [last_time + timedelta(hours=i+1) for i in range(prediction_hours)]
        
        # è½¬æ¢ä¸ºDataFrame
        pred_df = pd.DataFrame(predictions, index=future_times)
        
        print(f"âœ… é¢„æµ‹å®Œæˆï¼š{len(pred_df)} ä¸ªæ—¶é—´ç‚¹")
        
        return pred_df
    
    def predict_sequence(self, historical_data, prediction_hours=24):
        """
        å®Œæ•´çš„é¢„æµ‹æµç¨‹
        """
        print("ğŸš€ å¯åŠ¨AMDä¼˜åŒ–LSTMä»·æ ¼é¢„æµ‹")
        print("=" * 60)
        
        # æ•°æ®å‡†å¤‡
        train_data = self.prepare_data(historical_data)
        X_train, X_test, y_train_ohlc, y_test_ohlc, y_train_vol, y_test_vol, time_index = train_data
        
        # è®­ç»ƒæ¨¡å‹
        print("\nğŸ‹ï¸ è®­ç»ƒLSTMæ¨¡å‹...")
        train_losses, val_losses = self.train_model(
            X_train, y_train_ohlc, y_train_vol, 
            X_test, y_test_ohlc, y_test_vol,
            batch_size=64,  # AMD GPUä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
            epochs=150,
            learning_rate=0.001
        )
        
        # ç”Ÿæˆé¢„æµ‹
        predictions = self.predict_future(historical_data, prediction_hours)
        
        # è®¡ç®—æµ‹è¯•é›†å‡†ç¡®æ€§
        self.model.eval()
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(X_test).to(self.device)
            y_test_ohlc_tensor = torch.FloatTensor(y_test_ohlc).to(self.device)
            
            ohlc_pred_test, _ = self.model(X_test_tensor)
            
            # åæ ‡å‡†åŒ–
            y_test_unscaled = self.price_scaler.inverse_transform(y_test_ohlc_tensor.cpu().numpy())
            y_pred_unscaled = self.price_scaler.inverse_transform(ohlc_pred_test.cpu().numpy())
            
            # è®¡ç®—æŒ‡æ ‡
            mse = mean_squared_error(y_test_unscaled[:, 3], y_pred_unscaled[:, 3])  # æ”¶ç›˜ä»·MSE
            mae = mean_absolute_error(y_test_unscaled[:, 3], y_pred_unscaled[:, 3])  # æ”¶ç›˜ä»·MAE
            
            current_price = historical_data['close'].iloc[-1]
            predicted_price = predictions['close'].iloc[-1]
            change_pct = ((predicted_price - current_price) / current_price) * 100
        
        quality_metrics = {
            'mse': mse,
            'mae': mae,
            'current_price': current_price,
            'predicted_price': predicted_price,
            'change_pct': change_pct,
            'trend': 'ä¸Šæ¶¨' if change_pct > 1 else 'ä¸‹è·Œ' if change_pct < -1 else 'æ¨ªç›˜',
            'confidence': max(70, min(95, 90 - abs(change_pct)))
        }
        
        print(f"âœ… LSTMé¢„æµ‹å®Œæˆ!")
        print(f"   ğŸ“Š é¢„æµ‹ç‚¹æ•°: {len(predictions)}")
        print(f"   ğŸ’° å½“å‰ä»·æ ¼: {current_price:.6f}")
        print(f"   ğŸ¯ é¢„æµ‹ä»·æ ¼: {predicted_price:.6f}")
        print(f"   ğŸ“ˆ å˜åŒ–å¹…åº¦: {change_pct:+.2f}%")
        print(f"   ğŸ² MSE: {mse:.8f}")
        print(f"   ğŸ“ MAE: {mae:.6f}")
        
        return {
            "success": True,
            "predictions": predictions,
            "quality_metrics": quality_metrics,
            "model_info": {
                "model_type": "LSTM",
                "device": self.device_name,
                "sequence_length": self.sequence_length,
                "training_samples": len(X_train),
                "test_samples": len(X_test),
                "data_utilization": f"{(len(X_train) + len(X_test))/len(historical_data)*100:.1f}%"
            },
            "training_history": {
                "train_losses": train_losses,
                "val_losses": val_losses
            }
        }

# Streamlit WebUI
def main():
    """ä¸»WebUIåº”ç”¨"""
    st.set_page_config(
        page_title="AMDä¼˜åŒ–LSTMä»·æ ¼é¢„æµ‹ç³»ç»Ÿ",
        page_icon="ğŸš€",
        layout="wide"
    )
    
    st.title("ğŸš€ AMDä¼˜åŒ–LSTMä»·æ ¼é¢„æµ‹ç³»ç»Ÿ")
    st.markdown("**ä¸“ä¸ºAMD CPUå’ŒGPUä¼˜åŒ–ï¼Œå……åˆ†åˆ©ç”¨2000æ¡æ•°æ®çš„æ·±åº¦å­¦ä¹ é¢„æµ‹**")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ é¢„æµ‹é…ç½®")
        
        symbol = st.selectbox("é€‰æ‹©äº¤æ˜“å¯¹", [
            "DOGEUSDT", "BTCUSDT", "ETHUSDT", "ADAUSDT", 
            "DOTUSDT", "LINKUSDT", "LTCUSDT", "BCHUSDT",
            "XLMUSDT", "EOSUSDT", "MATICUSDT"
        ])
        
        sequence_length = st.slider("LSTMåºåˆ—é•¿åº¦", 20, 60, 30)
        prediction_hours = st.slider("é¢„æµ‹æ—¶é•¿ (å°æ—¶)", 1, 168, 24)
        
        if st.button("ğŸš€ å¼€å§‹LSTMé¢„æµ‹", type="primary"):
            st.session_state.run_prediction = True
    
    # ä¸»ç•Œé¢
    if st.session_state.get('run_prediction', False):
        run_lstm_prediction(symbol, sequence_length, prediction_hours)

def run_lstm_prediction(symbol, sequence_length, prediction_hours):
    """è¿è¡ŒLSTMé¢„æµ‹"""
    
    # æ˜¾ç¤ºæ•°æ®è·å–è¿›åº¦
    progress_container = st.container()
    with progress_container:
        st.info(f"ğŸ“Š æ­£åœ¨è·å– {symbol} çš„çœŸå®å†å²æ•°æ®...")
        progress_bar = st.progress(0)
        status_text = st.empty()
    
    try:
        # è·å–çœŸå®å†å²æ•°æ®
        status_text.text("ğŸ” ä»Huobiè·å–15åˆ†é’Ÿæ•°æ®...")
        progress_bar.progress(20)
        
        fetcher = MultiSourceDataFetcher(symbol)
        source, historical_data = fetcher.fetch_large_dataset(target_points=2000)
        
        if historical_data is None or len(historical_data) < 100:
            st.error("âŒ æ— æ³•è·å–è¶³å¤Ÿçš„å†å²æ•°æ®ï¼Œè¯·ç¨åé‡è¯•æˆ–æ›´æ¢äº¤æ˜“å¯¹")
            return
        
        progress_bar.progress(40)
        status_text.text(f"âœ… æ•°æ®è·å–æˆåŠŸ: {len(historical_data)}æ¡è®°å½•")
        
        # æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
        time_span = (historical_data.index[-1] - historical_data.index[0]).total_seconds() / 86400
        current_price = historical_data['close'].iloc[-1]
        
        # è®¾å¤‡ä¿¡æ¯
        device, device_name = setup_amd_device()
        
        with st.expander("ğŸ“Š æ•°æ®ä¸è®¾å¤‡ä¿¡æ¯"):
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ•°æ®é‡", f"{len(historical_data)}æ¡")
            with col2:
                st.metric("æ—¶é—´è·¨åº¦", f"{time_span:.1f}å¤©")
            with col3:
                st.metric("å½“å‰ä»·æ ¼", f"{current_price:.6f}")
            with col4:
                st.metric("è¿è¡Œè®¾å¤‡", device_name)
        
        # è¿è¡ŒLSTMé¢„æµ‹
        status_text.text("ğŸ§  æ­£åœ¨è®­ç»ƒLSTMæ¨¡å‹...")
        progress_bar.progress(60)
        
        with st.spinner("æ­£åœ¨è®­ç»ƒLSTMæ¨¡å‹ï¼Œè¯·ç¨å€™..."):
            predictor = AMDLSTMPredictionSystem(symbol, sequence_length)
            result = predictor.predict_sequence(historical_data, prediction_hours)
            
            progress_bar.progress(100)
            status_text.text("âœ… LSTMé¢„æµ‹å®Œæˆ!")
            
            if result and result["success"]:
                st.success("ğŸ‰ LSTMé¢„æµ‹ç”ŸæˆæˆåŠŸ!")
                
                # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                predictions = result["predictions"]
                metrics = result["quality_metrics"]
                model_info = result["model_info"]
                
                # æ ¸å¿ƒæŒ‡æ ‡
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("å½“å‰ä»·æ ¼", f"{metrics['current_price']:.6f}")
                with col2:
                    st.metric("é¢„æµ‹ä»·æ ¼", f"{metrics['predicted_price']:.6f}")
                with col3:
                    st.metric("å˜åŒ–å¹…åº¦", f"{metrics['change_pct']:+.2f}%")
                with col4:
                    st.metric("é¢„æµ‹è¶‹åŠ¿", metrics['trend'])
                
                # ç»˜åˆ¶é¢„æµ‹å›¾è¡¨
                fig = go.Figure()
                
                # å†å²ä»·æ ¼ (æœ€è¿‘100ä¸ªç‚¹)
                recent_data = historical_data.tail(100)
                fig.add_trace(go.Scatter(
                    x=recent_data.index,
                    y=recent_data['close'],
                    name="å†å²ä»·æ ¼",
                    line=dict(color='#1f77b4', width=2),
                    hovertemplate='æ—¶é—´: %{x}<br>ä»·æ ¼: %{y:.6f}<extra></extra>'
                ))
                
                # é¢„æµ‹ä»·æ ¼
                fig.add_trace(go.Scatter(
                    x=predictions.index,
                    y=predictions['close'],
                    name="LSTMé¢„æµ‹",
                    line=dict(color='#ff7f0e', dash='dash', width=2),
                    hovertemplate='æ—¶é—´: %{x}<br>é¢„æµ‹ä»·æ ¼: %{y:.6f}<extra></extra>'
                ))
                
                fig.update_layout(
                    title=f"ğŸ”® {symbol} LSTMä»·æ ¼é¢„æµ‹ (AMDä¼˜åŒ–)",
                    xaxis_title="æ—¶é—´",
                    yaxis_title="ä»·æ ¼ (USDT)",
                    height=600,
                    showlegend=True,
                    hovermode='x unified'
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # æ¨¡å‹è¯¦æƒ…
                col1, col2 = st.columns(2)
                with col1:
                    st.subheader("ğŸ“ˆ é¢„æµ‹åˆ†æ")
                    st.write(f"**é¢„æµ‹è¶‹åŠ¿**: {metrics['trend']}")
                    st.write(f"**ç½®ä¿¡åº¦**: {metrics['confidence']:.1f}%")
                    st.write(f"**MSE**: {metrics['mse']:.8f}")
                    st.write(f"**MAE**: {metrics['mae']:.6f}")
                
                with col2:
                    st.subheader("ğŸ¯ æ¨¡å‹ä¿¡æ¯")
                    st.write(f"**æ¨¡å‹ç±»å‹**: {model_info['model_type']}")
                    st.write(f"**è¿è¡Œè®¾å¤‡**: {model_info['device']}")
                    st.write(f"**åºåˆ—é•¿åº¦**: {model_info['sequence_length']}")
                    st.write(f"**è®­ç»ƒæ ·æœ¬**: {model_info['training_samples']}")
                    st.write(f"**æ•°æ®åˆ©ç”¨ç‡**: {model_info['data_utilization']}")
                
                # è®­ç»ƒå†å²
                if "training_history" in result:
                    with st.expander("ğŸ“ˆ è®­ç»ƒå†å²"):
                        train_losses = result["training_history"]["train_losses"]
                        val_losses = result["training_history"]["val_losses"]
                        
                        fig_loss = go.Figure()
                        fig_loss.add_trace(go.Scatter(
                            y=train_losses,
                            name="è®­ç»ƒæŸå¤±",
                            line=dict(color='blue')
                        ))
                        fig_loss.add_trace(go.Scatter(
                            y=val_losses,
                            name="éªŒè¯æŸå¤±",
                            line=dict(color='red')
                        ))
                        fig_loss.update_layout(
                            title="LSTMè®­ç»ƒæŸå¤±æ›²çº¿",
                            xaxis_title="è®­ç»ƒè½®æ¬¡",
                            yaxis_title="æŸå¤±å€¼",
                            height=400
                        )
                        st.plotly_chart(fig_loss, use_container_width=True)
                
                # é¢„æµ‹æ•°æ®è¡¨æ ¼
                with st.expander("ğŸ“‹ è¯¦ç»†é¢„æµ‹æ•°æ®"):
                    st.dataframe(predictions.round(6))
            else:
                st.error("âŒ LSTMé¢„æµ‹ç”Ÿæˆå¤±è´¥!")
                
    except Exception as e:
        st.error(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        
    finally:
        # æ¸…ç†è¿›åº¦æ˜¾ç¤º
        progress_container.empty()
    
    st.session_state.run_prediction = False

if __name__ == "__main__":
    main() 