"""
ä¼˜åŒ–ç‰ˆLSTMä»·æ ¼é¢„æµ‹ç³»ç»Ÿ v2.0
- æ•°æ®å±‚ï¼šç»Ÿä¸€é‡é‡‡æ ·ã€æŠ€æœ¯æŒ‡æ ‡ã€RobustScaler
- æ¨¡å‹æ¶æ„ï¼šåŒå‘LSTM + Attentionæœºåˆ¶  
- è®­ç»ƒç­–ç•¥ï¼šæ»šåŠ¨äº¤å‰éªŒè¯ã€ä¼˜åŒ–å™¨æ”¹è¿›
- CPUä¼˜åŒ–ï¼šMKL-DNNã€torch.compileåŠ é€Ÿ
"""
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit
import torch.nn.functional as F

# å°è¯•å¯¼å…¥TA-Libï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨pandaså¤‡ç”¨å®ç°
try:
    import talib
    TALIB_AVAILABLE = True
    print("âœ… TA-Libå·²å¯¼å…¥")
except ImportError:
    TALIB_AVAILABLE = False
    print("âš ï¸ TA-Libæœªå®‰è£…ï¼Œå°†ä½¿ç”¨pandaså¤‡ç”¨å®ç°")

from datetime import datetime, timedelta
import warnings
import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import time
import os
import joblib
from multi_source_fetcher import MultiSourceDataFetcher

warnings.filterwarnings('ignore')

# CPUæ€§èƒ½ä¼˜åŒ–è®¾ç½®
def setup_optimized_device():
    """è®¾ç½®ä¼˜åŒ–çš„è®¾å¤‡é…ç½®"""
    device = "cpu"  # é»˜è®¤CPU
    device_name = "ä¼˜åŒ–CPU"
    
    # å¯ç”¨MKL-DNN/OneDNN (é’ˆå¯¹Intel/AMD CPUä¼˜åŒ–)
    try:
        torch.backends.mkldnn.enabled = True
        print("ğŸš€ å¯ç”¨MKL-DNNåŠ é€Ÿ")
    except:
        pass
    
    if torch.cuda.is_available():
        device = "cuda"
        device_name = torch.cuda.get_device_name(0)
        print(f"ğŸš€ æ£€æµ‹åˆ°GPU: {device_name}")
        torch.backends.cudnn.benchmark = True
        torch.cuda.empty_cache()
    else:
        print("ğŸ”§ ä½¿ç”¨CPUä¼˜åŒ–è®¾ç½®...")
        # å¤šæ ¸å¹¶è¡Œè®¾ç½®
        torch.set_num_threads(os.cpu_count())
        # è®¾ç½®é»˜è®¤æ•°æ®ç±»å‹ä¸ºbfloat16 (å¦‚æœæ”¯æŒ)
        if hasattr(torch, 'backends') and hasattr(torch.backends, 'cpu'):
            try:
                torch.set_default_dtype(torch.float32)  # ä¿æŒç²¾åº¦
            except:
                pass
                
    return device, device_name

class EnhancedLSTMPredictor(nn.Module):
    """
    å¢å¼ºçš„LSTMé¢„æµ‹æ¨¡å‹
    - åŒå‘LSTM
    - Multi-Head Attentionæœºåˆ¶
    - LayerNormå’ŒResidualè¿æ¥
    - å¤šä»»åŠ¡å­¦ä¹ ï¼ˆä»·æ ¼å›å½’ + æ–¹å‘åˆ†ç±»ï¼‰
    """
    def __init__(self, input_size=15, hidden_size=128, num_layers=3, dropout=0.2, 
                 num_heads=4, output_size=4):
        super(EnhancedLSTMPredictor, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.input_size = input_size
        
        # åŒå‘LSTM
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
            bidirectional=True  # åŒå‘LSTM
        )
        
        # Multi-Head Attention
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,  # åŒå‘LSTMè¾“å‡ºç»´åº¦
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # Layer Normalization
        self.layer_norm = nn.LayerNorm(hidden_size * 2)
        
        # Dropoutå±‚
        self.dropout = nn.Dropout(dropout)
        
        # ä»·æ ¼é¢„æµ‹åˆ†æ”¯ï¼ˆå›å½’ä»»åŠ¡ï¼‰
        self.price_branch = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size)  # OHLC
        )
        
        # æˆäº¤é‡é¢„æµ‹åˆ†æ”¯
        self.volume_branch = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, 1),
            nn.ReLU()  # ç¡®ä¿æˆäº¤é‡ä¸ºæ­£
        )
        
        # æ–¹å‘åˆ†ç±»åˆ†æ”¯ï¼ˆå¤šä»»åŠ¡å­¦ä¹ ï¼‰
        self.direction_branch = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, 2),  # up/downäºŒåˆ†ç±»
            nn.Softmax(dim=-1)
        )
        
    def forward(self, x):
        batch_size = x.size(0)
        seq_len = x.size(1)
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€ï¼ˆåŒå‘LSTMéœ€è¦2å€æ•°é‡ï¼‰
        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, device=x.device)
        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, device=x.device)
        
        # åŒå‘LSTMå‰å‘ä¼ æ’­
        lstm_out, _ = self.lstm(x, (h0, c0))
        
        # Attentionæœºåˆ¶
        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # Residualè¿æ¥ + LayerNorm
        lstm_out = self.layer_norm(lstm_out + attn_output)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]
        last_output = self.dropout(last_output)
        
        # å¤šåˆ†æ”¯è¾“å‡º
        price_pred = self.price_branch(last_output)
        volume_pred = self.volume_branch(last_output)
        direction_pred = self.direction_branch(last_output)
        
        return price_pred, volume_pred, direction_pred

class FocalLoss(nn.Module):
    """å¤šåˆ†ç±» Focal Loss (softmax ç‰ˆæœ¬)"""
    def __init__(self, alpha: float = 0.25, gamma: float = 2.0, reduction: str = 'mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor):
        # inputs å½¢çŠ¶: (batch, num_classes) -> softmax æ¦‚ç‡
        eps = 1e-8
        inputs = inputs.clamp(min=eps, max=1.0-eps)
        one_hot = F.one_hot(targets.view(-1), num_classes=inputs.size(1)).float().to(inputs.device)
        pt = (inputs * one_hot).sum(dim=1)  # å–æ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡
        focal_term = self.alpha * (1 - pt) ** self.gamma
        loss = -focal_term * pt.log()
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

class OptimizedLSTMSystem:
    """ä¼˜åŒ–çš„LSTMä»·æ ¼é¢„æµ‹ç³»ç»Ÿ v2.0"""
    
    def __init__(self, symbol="DOGEUSDT", sequence_length=30):
        self.symbol = symbol
        self.sequence_length = sequence_length
        self.device, self.device_name = setup_optimized_device()
        
        # ä½¿ç”¨RobustScalerï¼ˆå¯¹ç¦»ç¾¤å€¼æ›´ç¨³å¥ï¼‰
        self.price_scaler = RobustScaler()
        self.volume_scaler = RobustScaler()
        self.tech_scaler = StandardScaler()  # æŠ€æœ¯æŒ‡æ ‡ä½¿ç”¨StandardScaler
        
        # æ¨¡å‹
        self.model = None
        self.trained = False
        
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.scaler_path = f"scalers_{symbol}.joblib"
        
        print(f"ğŸ“± è¿è¡Œè®¾å¤‡: {self.device_name}")
        print(f"ğŸ”¢ åºåˆ—é•¿åº¦: {sequence_length}")
        
    def add_technical_indicators(self, data):
        """
        æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ - æ”¯æŒTA-Libå’Œpandaså¤‡ç”¨å®ç°
        """
        print("ğŸ“Š è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
        
        df = data.copy()
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        try:
            # 1. å¯¹æ•°æ”¶ç›Šç‡
            df['log_return'] = np.log(df['close'] / df['close'].shift(1))
            
            # 2. æˆäº¤é‡Z-score
            df['volume_zscore'] = (df['volume'] - df['volume'].rolling(20).mean()) / df['volume'].rolling(20).std()
            
            if TALIB_AVAILABLE:
                # ä½¿ç”¨TA-Libè®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                print("   ğŸš€ ä½¿ç”¨TA-Libè®¡ç®—æŠ€æœ¯æŒ‡æ ‡")
                
                # 3. EMAæŒ‡æ ‡
                df['ema_12'] = talib.EMA(df['close'].values, timeperiod=12)
                df['ema_26'] = talib.EMA(df['close'].values, timeperiod=26)
                
                # 4. MACD
                df['macd'], df['macd_signal'], df['macd_hist'] = talib.MACD(df['close'].values)
                
                # 5. RSI
                df['rsi'] = talib.RSI(df['close'].values, timeperiod=14)
                
                # 6. å¸ƒæ—å¸¦
                df['bb_upper'], df['bb_middle'], df['bb_lower'] = talib.BBANDS(df['close'].values)
                df['bb_percent'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
                
                # 7. ATR (å¹³å‡çœŸå®èŒƒå›´)
                df['atr'] = talib.ATR(df['high'].values, df['low'].values, df['close'].values)
                
            else:
                # ä½¿ç”¨pandaså¤‡ç”¨å®ç°
                print("   ğŸ”§ ä½¿ç”¨pandaså¤‡ç”¨å®ç°æŠ€æœ¯æŒ‡æ ‡")
                
                # 3. EMAæŒ‡æ ‡ (pandaså®ç°)
                df['ema_12'] = df['close'].ewm(span=12, adjust=False).mean()
                df['ema_26'] = df['close'].ewm(span=26, adjust=False).mean()
                
                # 4. MACD (ç®€åŒ–å®ç°)
                df['macd'] = df['ema_12'] - df['ema_26']
                df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
                df['macd_hist'] = df['macd'] - df['macd_signal']
                
                # 5. RSI (pandaså®ç°)
                delta = df['close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
                rs = gain / loss
                df['rsi'] = 100 - (100 / (1 + rs))
                
                # 6. å¸ƒæ—å¸¦ (pandaså®ç°)
                df['bb_middle'] = df['close'].rolling(window=20).mean()
                bb_std = df['close'].rolling(window=20).std()
                df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
                df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
                df['bb_percent'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
                
                # 7. ATR (pandaså®ç°)
                high_low = df['high'] - df['low']
                high_close = np.abs(df['high'] - df['close'].shift())
                low_close = np.abs(df['low'] - df['close'].shift())
                true_range = np.maximum(high_low, np.maximum(high_close, low_close))
                df['atr'] = true_range.rolling(window=14).mean()
            
            # 8. æˆäº¤é‡ç§»åŠ¨å¹³å‡ (é€šç”¨å®ç°)
            df['volume_ma'] = df['volume'].rolling(10).mean()
            df['volume_ratio'] = df['volume'] / df['volume_ma']
            
            # 9. å‘¨æœŸæ—¶é—´ç‰¹å¾ï¼ˆäº¤æ˜“æ—¥è§„å¾‹ï¼‰
            df['minute_of_day'] = df.index.hour * 60 + df.index.minute
            df['minute_sin'] = np.sin(2 * np.pi * df['minute_of_day'] / 1440)
            df['minute_cos'] = np.cos(2 * np.pi * df['minute_of_day'] / 1440)

            df['dayofweek'] = df.index.dayofweek
            df['day_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
            df['day_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)

            # 10. èœ¡çƒ›å›¾å½¢æ€ç‰¹å¾
            df['body'] = (df['close'] - df['open']) / df['open'].replace(0, np.nan)
            df['upper_shadow'] = (df['high'] - df[['close', 'open']].max(axis=1)) / df['open'].replace(0, np.nan)
            df['lower_shadow'] = (df[['close', 'open']].min(axis=1) - df['low']) / df['open'].replace(0, np.nan)

            # 11. VWAP å·®å€¼
            df['vwap'] = (df['high'] + df['low'] + df['close']) / 3
            df['vwap_diff'] = (df['close'] - df['vwap']) / df['vwap']
            
            # åˆ é™¤åŒ…å«NaNçš„è¡Œ
            df = df.dropna()
            
            indicator_type = "TA-Lib" if TALIB_AVAILABLE else "pandaså¤‡ç”¨"
            print(f"âœ… æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆ({indicator_type})ï¼Œæ•°æ®é‡: {len(df)}")
            
            return df
            
        except Exception as e:
            print(f"âŒ æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥: {str(e)}")
            # å¦‚æœæŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥ï¼Œè¿”å›åŸºç¡€æ•°æ®
            print("   ğŸ”„ å›é€€åˆ°åŸºç¡€OHLCVæ•°æ®")
            return data
    
    def resample_and_clean_data(self, data):
        """
        ç»Ÿä¸€é‡é‡‡æ ·åˆ°15åˆ†é’Ÿï¼Œæ¸…ç†é‡å¤æ—¶é—´æˆ³
        """
        print("ğŸ”§ ç»Ÿä¸€é‡é‡‡æ ·å’Œæ•°æ®æ¸…ç†...")
        
        try:
            # ç¡®ä¿ç´¢å¼•æ˜¯æ—¶é—´æˆ³
            if not isinstance(data.index, pd.DatetimeIndex):
                data.index = pd.to_datetime(data.index)
            
            # åˆ é™¤é‡å¤çš„æ—¶é—´æˆ³ï¼Œä¿ç•™æœ€åä¸€ä¸ª
            data = data[~data.index.duplicated(keep='last')]
            
            # é‡é‡‡æ ·åˆ°15åˆ†é’Ÿ
            resampled = data.resample('15T').agg({
                'open': 'first',
                'high': 'max', 
                'low': 'min',
                'close': 'last',
                'volume': 'sum'
            })
            
            # å‰å‘å¡«å……ç©ºå€¼
            resampled = resampled.fillna(method='ffill')
            
            # åˆ é™¤ä»ç„¶åŒ…å«NaNçš„è¡Œ
            resampled = resampled.dropna()
            
            print(f"âœ… é‡é‡‡æ ·å®Œæˆ: {len(data)} -> {len(resampled)} æ¡è®°å½•")
            
            return resampled
            
        except Exception as e:
            print(f"âŒ é‡é‡‡æ ·å¤±è´¥: {str(e)}")
            return data
    
    def prepare_enhanced_data(self, data, use_walk_forward=True):
        """
        å¢å¼ºçš„æ•°æ®å‡†å¤‡ï¼ŒåŒ…å«æŠ€æœ¯æŒ‡æ ‡å’Œæ»šåŠ¨äº¤å‰éªŒè¯
        """
        print(f"ğŸ“Š å‡†å¤‡å¢å¼ºæ•°æ®: {len(data)} æ¡è®°å½•")
        
        # 1. ç»Ÿä¸€é‡é‡‡æ ·å’Œæ¸…ç†
        data = self.resample_and_clean_data(data)
        
        # 2. æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
        data = self.add_technical_indicators(data)
        
        if len(data) < self.sequence_length + 50:
            raise ValueError(f"æ•°æ®é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {self.sequence_length + 50} æ¡è®°å½•")
        
        # 3. å‡†å¤‡ç‰¹å¾
        # åŸºç¡€OHLCV
        ohlc_data = data[['open', 'high', 'low', 'close']].values
        volume_data = data[['volume']].values
        
        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ï¼ˆæ‰©å……åå…±17åˆ—ï¼‰
        tech_cols = ['log_return', 'volume_zscore', 'ema_12', 'ema_26',
                     'macd', 'rsi', 'bb_percent', 'atr', 'volume_ratio',
                     'minute_sin', 'minute_cos', 'day_sin', 'day_cos',
                     'body', 'upper_shadow', 'lower_shadow', 'vwap_diff']
        tech_features = data[tech_cols].values
        
        # 4. æ•°æ®æ ‡å‡†åŒ–
        ohlc_scaled = self.price_scaler.fit_transform(ohlc_data)
        volume_scaled = self.volume_scaler.fit_transform(volume_data)
        tech_scaled = self.tech_scaler.fit_transform(tech_features)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾ (4 + 1 + 17 = 22ç»´)
        combined_features = np.concatenate([ohlc_scaled, volume_scaled, tech_scaled], axis=1)
        
        # 5. åˆ›å»ºåºåˆ—æ•°æ®
        sequences = []
        targets_ohlc = []
        targets_volume = []
        targets_direction = []  # æ–¹å‘æ ‡ç­¾
        
        for i in range(self.sequence_length, len(combined_features)):
            # è¾“å…¥åºåˆ—
            seq = combined_features[i-self.sequence_length:i]
            sequences.append(seq)
            
            # ç›®æ ‡å€¼
            targets_ohlc.append(ohlc_scaled[i])
            targets_volume.append(volume_scaled[i])
            
            # æ–¹å‘æ ‡ç­¾ï¼ˆä¸Šæ¶¨=1ï¼Œä¸‹è·Œ=0ï¼‰
            current_close = data['close'].iloc[i]
            prev_close = data['close'].iloc[i-1]
            direction = 1 if current_close > prev_close else 0
            targets_direction.append(direction)
        
        sequences = np.array(sequences)
        targets_ohlc = np.array(targets_ohlc)
        targets_volume = np.array(targets_volume)
        targets_direction = np.array(targets_direction)
        
        print(f"âœ… ç”Ÿæˆåºåˆ—: {len(sequences)} ä¸ª")
        print(f"ğŸ“ ç‰¹å¾ç»´åº¦: {sequences.shape[2]} (OHLC:4 + Volume:1 + æŠ€æœ¯æŒ‡æ ‡:{len(tech_cols)})")
        
        if use_walk_forward:
            # ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            print("ğŸ”„ ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯...")
            tscv = TimeSeriesSplit(n_splits=5)
            
            # è¿”å›æ‰€æœ‰æŠ˜å çš„æ•°æ®ï¼Œç”¨äºè®­ç»ƒæ—¶é€‰æ‹©
            cv_splits = []
            for train_idx, test_idx in tscv.split(sequences):
                cv_splits.append({
                    'X_train': sequences[train_idx],
                    'X_test': sequences[test_idx],
                    'y_train_ohlc': targets_ohlc[train_idx],
                    'y_test_ohlc': targets_ohlc[test_idx],
                    'y_train_vol': targets_volume[train_idx],
                    'y_test_vol': targets_volume[test_idx],
                    'y_train_dir': targets_direction[train_idx],
                    'y_test_dir': targets_direction[test_idx]
                })
            
            return cv_splits, data.index[self.sequence_length:]
        else:
            # ä¼ ç»Ÿå›ºå®šåˆ†å‰²
            split_idx = int(len(sequences) * 0.85)
            
            return [{
                'X_train': sequences[:split_idx],
                'X_test': sequences[split_idx:],
                'y_train_ohlc': targets_ohlc[:split_idx],
                'y_test_ohlc': targets_ohlc[split_idx:],
                'y_train_vol': targets_volume[:split_idx],
                'y_test_vol': targets_volume[split_idx:],
                'y_train_dir': targets_direction[:split_idx],
                'y_test_dir': targets_direction[split_idx:]
            }], data.index[self.sequence_length:]
    
    def create_enhanced_model(self, input_size=22):
        """åˆ›å»ºå¢å¼ºçš„LSTMæ¨¡å‹"""
        model = EnhancedLSTMPredictor(
            input_size=input_size,
            hidden_size=128,
            num_layers=3,
            dropout=0.2,
            num_heads=4,
            output_size=4
        )
        
        # å°è¯•ä½¿ç”¨torch.compileä¼˜åŒ–ï¼ˆPyTorch 2.xï¼‰
        # Windows è‹¥æœªå®‰è£… Visual Studio Build Tools (cl.exe) ä¼šæŠ¥é”™ï¼Œæ£€æµ‹åå†å†³å®š
        try:
            import shutil, platform
            has_cl = shutil.which('cl') is not None
            if hasattr(torch, 'compile') and (platform.system() != 'Windows' or has_cl):
                model = torch.compile(model, mode="max-autotune")
                print("ğŸš€ å¯ç”¨torch.compileä¼˜åŒ–")
            else:
                print("âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨ C++ ç¼–è¯‘å™¨ï¼Œå·²è·³è¿‡ torch.compile")
        except Exception as e:
            print(f"âš ï¸ torch.compile è·³è¿‡: {e}")
            
        return model.to(self.device)
    
    def train_enhanced_model(self, cv_splits, batch_size=32, epochs=100, learning_rate=0.001):
        """
        å¢å¼ºçš„æ¨¡å‹è®­ç»ƒï¼Œæ”¯æŒæ»šåŠ¨äº¤å‰éªŒè¯
        """
        print("ğŸš€ å¼€å§‹å¢å¼ºè®­ç»ƒ...")
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªåˆ†å‰²ä½œä¸ºä¸»è¦è®­ç»ƒæ•°æ®
        main_split = cv_splits[-1]
        
        # åˆ›å»ºæ¨¡å‹
        input_size = main_split['X_train'].shape[2]
        self.model = self.create_enhanced_model(input_size=input_size)
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_train = torch.FloatTensor(main_split['X_train']).to(self.device)
        y_train_ohlc = torch.FloatTensor(main_split['y_train_ohlc']).to(self.device)
        y_train_vol = torch.FloatTensor(main_split['y_train_vol']).to(self.device)
        y_train_dir = torch.LongTensor(main_split['y_train_dir']).to(self.device)
        
        X_test = torch.FloatTensor(main_split['X_test']).to(self.device)
        y_test_ohlc = torch.FloatTensor(main_split['y_test_ohlc']).to(self.device)
        y_test_vol = torch.FloatTensor(main_split['y_test_vol']).to(self.device)
        y_test_dir = torch.LongTensor(main_split['y_test_dir']).to(self.device)
        
        # æ•°æ®åŠ è½½å™¨ï¼ˆå¤šè¿›ç¨‹ä¼˜åŒ–ï¼‰
        train_dataset = TensorDataset(X_train, y_train_ohlc, y_train_vol, y_train_dir)
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=min(4, os.cpu_count()//2),  # CPUä¼˜åŒ–çš„workeræ•°é‡
            persistent_workers=True,
            pin_memory=True if self.device == "cuda" else False
        )
        
        # æŸå¤±å‡½æ•°
        criterion_price = nn.MSELoss()
        criterion_volume = nn.MSELoss()
        criterion_direction = FocalLoss(alpha=0.25, gamma=2.0)
        
        # ä¼˜åŒ–å™¨æ”¹è¿›: AdamW + Lookahead
        base_optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=learning_rate, 
            weight_decay=1e-5,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨: CosineAnnealingWarmRestarts
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            base_optimizer, 
            T_0=20,  # é‡å¯å‘¨æœŸ
            eta_min=learning_rate * 0.01
        )
        
        # è®­ç»ƒå¾ªç¯
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        patience = 30  # å¢åŠ patience
        patience_counter = 0
        
        print(f"ğŸ”§ è®¾å¤‡: {self.device_name}")
        print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"ğŸ”„ è®¡åˆ’è®­ç»ƒè½®æ•°: {epochs}")
        
        for epoch in range(epochs):
            # è®­ç»ƒæ¨¡å¼
            self.model.train()
            epoch_train_loss = 0
            
            for batch_idx, (batch_x, batch_y_ohlc, batch_y_vol, batch_y_dir) in enumerate(train_loader):
                base_optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                price_pred, vol_pred, dir_pred = self.model(batch_x)
                
                # å¤šä»»åŠ¡æŸå¤±
                loss_price = criterion_price(price_pred, batch_y_ohlc)
                loss_vol = criterion_volume(vol_pred.squeeze(), batch_y_vol.squeeze())
                loss_dir = criterion_direction(dir_pred, batch_y_dir)
                
                total_loss = loss_price + 0.1 * loss_vol + 0.1 * loss_dir
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                base_optimizer.step()
                epoch_train_loss += total_loss.item()
            
            # éªŒè¯
            self.model.eval()
            with torch.no_grad():
                price_pred_val, vol_pred_val, dir_pred_val = self.model(X_test)
                val_loss_price = criterion_price(price_pred_val, y_test_ohlc)
                val_loss_vol = criterion_volume(vol_pred_val.squeeze(), y_test_vol.squeeze())
                val_loss_dir = criterion_direction(dir_pred_val, y_test_dir)
                val_loss = val_loss_price + 0.1 * val_loss_vol + 0.1 * val_loss_dir
            
            avg_train_loss = epoch_train_loss / len(train_loader)
            train_losses.append(avg_train_loss)
            val_losses.append(val_loss.item())
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            
            # æ—©åœæ£€æŸ¥ï¼ˆä½¿ç”¨7æ—¥æ»‘åŠ¨å¹³å‡ï¼‰
            if len(val_losses) >= 7:
                recent_val_loss = np.mean(val_losses[-7:])
                if recent_val_loss < best_val_loss:
                    best_val_loss = recent_val_loss
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œscaler
                    torch.save(self.model.state_dict(), f'best_lstm_model_{self.symbol}.pth')
                    self._save_scalers()
                else:
                    patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                current_lr = scheduler.get_last_lr()[0]
                print(f"è½®æ¬¡ [{epoch+1}/{epochs}] - "
                      f"è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}, "
                      f"éªŒè¯æŸå¤±: {val_loss:.6f}, "
                      f"å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œç¬¬ {epoch+1} è½®")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        self.model.load_state_dict(torch.load(f'best_lstm_model_{self.symbol}.pth'))
        self.trained = True
        
        print(f"âœ… å¢å¼ºè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        return train_losses, val_losses
    
    def _save_scalers(self):
        """ä¿å­˜æ ‡å‡†åŒ–å™¨"""
        scalers = {
            'price_scaler': self.price_scaler,
            'volume_scaler': self.volume_scaler,
            'tech_scaler': self.tech_scaler
        }
        joblib.dump(scalers, self.scaler_path)
    
    def _load_scalers(self):
        """åŠ è½½æ ‡å‡†åŒ–å™¨"""
        try:
            scalers = joblib.load(self.scaler_path)
            self.price_scaler = scalers['price_scaler']
            self.volume_scaler = scalers['volume_scaler'] 
            self.tech_scaler = scalers['tech_scaler']
            return True
        except:
            return False

    def predict_enhanced_future(self, data, prediction_hours=24):
        """å¢å¼ºçš„æœªæ¥ä»·æ ¼é¢„æµ‹"""
        if not self.trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        print(f"ğŸ”® é¢„æµ‹æœªæ¥ {prediction_hours} å°æ—¶ä»·æ ¼...")
        
        self.model.eval()
        
        # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
        data_with_tech = self.add_technical_indicators(data)
        
        # å‡†å¤‡ç‰¹å¾
        ohlc_data = data_with_tech[['open', 'high', 'low', 'close']].values
        volume_data = data_with_tech[['volume']].values
        tech_cols = ['log_return', 'volume_zscore', 'ema_12', 'ema_26',
                     'macd', 'rsi', 'bb_percent', 'atr', 'volume_ratio',
                     'minute_sin', 'minute_cos', 'day_sin', 'day_cos',
                     'body', 'upper_shadow', 'lower_shadow', 'vwap_diff']
        tech_features = data_with_tech[tech_cols].values
        
        # æ ‡å‡†åŒ–
        ohlc_scaled = self.price_scaler.transform(ohlc_data)
        volume_scaled = self.volume_scaler.transform(volume_data)
        tech_scaled = self.tech_scaler.transform(tech_features)
        
        combined_features = np.concatenate([ohlc_scaled, volume_scaled, tech_scaled], axis=1)
        
        # æœ€åä¸€ä¸ªåºåˆ—
        last_sequence = combined_features[-self.sequence_length:]
        current_sequence = torch.FloatTensor(last_sequence).unsqueeze(0).to(self.device)
        
        predictions = []
        directions = []
        
        with torch.no_grad():
            for _ in range(prediction_hours):
                # é¢„æµ‹
                price_pred, vol_pred, dir_pred = self.model(current_sequence)
                
                # åæ ‡å‡†åŒ–ä»·æ ¼å’Œæˆäº¤é‡
                price_pred_unscaled = self.price_scaler.inverse_transform(price_pred.cpu().numpy())
                vol_pred_unscaled = self.volume_scaler.inverse_transform(vol_pred.cpu().numpy())
                
                # æ–¹å‘é¢„æµ‹
                direction_prob = torch.softmax(dir_pred, dim=-1).cpu().numpy()[0]
                predicted_direction = "ä¸Šæ¶¨" if direction_prob[1] > 0.5 else "ä¸‹è·Œ"
                direction_confidence = max(direction_prob)
                
                # ç¡®ä¿ä»·æ ¼é€»è¾‘æ€§
                open_price = price_pred_unscaled[0][0]
                high_price = max(price_pred_unscaled[0][1], open_price, price_pred_unscaled[0][3])
                low_price = min(price_pred_unscaled[0][2], open_price, price_pred_unscaled[0][3])
                close_price = price_pred_unscaled[0][3]
                volume = max(0, vol_pred_unscaled[0][0])
                
                predictions.append({
                    'open': open_price,
                    'high': high_price,
                    'low': low_price,
                    'close': close_price,
                    'volume': volume
                })
                
                directions.append({
                    'direction': predicted_direction,
                    'confidence': direction_confidence
                })
                
                # æ›´æ–°åºåˆ—ï¼ˆè¿™é‡Œéœ€è¦è®¡ç®—æ–°çš„æŠ€æœ¯æŒ‡æ ‡ï¼Œç®€åŒ–å¤„ç†ï¼‰
                new_point = np.array([[open_price, high_price, low_price, close_price, volume]])
                new_point_scaled = np.concatenate([
                    self.price_scaler.transform(new_point[:, :4]),
                    self.volume_scaler.transform(new_point[:, 4:5]),
                    np.zeros((1, 17))  # æŠ€æœ¯æŒ‡æ ‡æš‚æ—¶ç”¨0å¡«å……
                ], axis=1)
                
                # æ»‘åŠ¨çª—å£æ›´æ–°
                current_sequence = torch.cat([
                    current_sequence[:, 1:, :],
                    torch.FloatTensor(new_point_scaled).unsqueeze(0).to(self.device)
                ], dim=1)
        
        # ç”Ÿæˆæ—¶é—´ç´¢å¼•
        last_time = data.index[-1]
        future_times = [last_time + timedelta(hours=i+1) for i in range(prediction_hours)]
        
        # è½¬æ¢ä¸ºDataFrame
        pred_df = pd.DataFrame(predictions, index=future_times)
        dir_df = pd.DataFrame(directions, index=future_times)
        
        print(f"âœ… å¢å¼ºé¢„æµ‹å®Œæˆï¼š{len(pred_df)} ä¸ªæ—¶é—´ç‚¹")
        
        return pred_df, dir_df

    def enhanced_predict_sequence(self, historical_data, prediction_hours=24):
        """å®Œæ•´çš„å¢å¼ºé¢„æµ‹æµç¨‹"""
        print("ğŸš€ å¯åŠ¨å¢å¼ºLSTMä»·æ ¼é¢„æµ‹ç³»ç»Ÿ v2.0")
        print("=" * 60)
        
        # æ•°æ®å‡†å¤‡
        cv_splits, time_index = self.prepare_enhanced_data(historical_data, use_walk_forward=True)
        
        # è®­ç»ƒæ¨¡å‹
        print("\nğŸ‹ï¸ è®­ç»ƒå¢å¼ºLSTMæ¨¡å‹...")
        train_losses, val_losses = self.train_enhanced_model(
            cv_splits,
            batch_size=64,
            epochs=150,
            learning_rate=0.001
        )
        
        # ç”Ÿæˆé¢„æµ‹
        predictions, directions = self.predict_enhanced_future(historical_data, prediction_hours)
        
        # è®¡ç®—å¢å¼ºæŒ‡æ ‡
        main_split = cv_splits[-1]
        self.model.eval()
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(main_split['X_test']).to(self.device)
            y_test_ohlc_tensor = torch.FloatTensor(main_split['y_test_ohlc']).to(self.device)
            y_test_dir_tensor = torch.LongTensor(main_split['y_test_dir']).to(self.device)
            
            price_pred_test, _, dir_pred_test = self.model(X_test_tensor)
            
            # åæ ‡å‡†åŒ–
            y_test_unscaled = self.price_scaler.inverse_transform(y_test_ohlc_tensor.cpu().numpy())
            y_pred_unscaled = self.price_scaler.inverse_transform(price_pred_test.cpu().numpy())
            
            # ä»·æ ¼æŒ‡æ ‡
            mse = mean_squared_error(y_test_unscaled[:, 3], y_pred_unscaled[:, 3])
            mae = mean_absolute_error(y_test_unscaled[:, 3], y_pred_unscaled[:, 3])
            mape = np.mean(np.abs((y_test_unscaled[:, 3] - y_pred_unscaled[:, 3]) / y_test_unscaled[:, 3])) * 100
            
            # æ–¹å‘å‡†ç¡®ç‡
            dir_pred_classes = torch.argmax(dir_pred_test, dim=1)
            directional_accuracy = (dir_pred_classes == y_test_dir_tensor).float().mean().item() * 100
            
            current_price = historical_data['close'].iloc[-1]
            predicted_price = predictions['close'].iloc[-1]
            change_pct = ((predicted_price - current_price) / current_price) * 100
        
        quality_metrics = {
            'mse': mse,
            'mae': mae,
            'mape': mape,
            'directional_accuracy': directional_accuracy,
            'current_price': current_price,
            'predicted_price': predicted_price,
            'change_pct': change_pct,
            'trend': 'ä¸Šæ¶¨' if change_pct > 1 else 'ä¸‹è·Œ' if change_pct < -1 else 'æ¨ªç›˜',
            'confidence': max(70, min(95, 90 - abs(change_pct))),
            'main_direction': directions['direction'].iloc[-1],
            'direction_confidence': directions['confidence'].iloc[-1]
        }
        
        print(f"âœ… å¢å¼ºLSTMé¢„æµ‹å®Œæˆ!")
        print(f"   ğŸ“Š é¢„æµ‹ç‚¹æ•°: {len(predictions)}")
        print(f"   ğŸ’° å½“å‰ä»·æ ¼: {current_price:.6f}")
        print(f"   ğŸ¯ é¢„æµ‹ä»·æ ¼: {predicted_price:.6f}")
        print(f"   ğŸ“ˆ å˜åŒ–å¹…åº¦: {change_pct:+.2f}%")
        print(f"   ğŸ² MSE: {mse:.8f}")
        print(f"   ğŸ“ MAE: {mae:.6f}")
        print(f"   ğŸ“Š MAPE: {mape:.2f}%")
        print(f"   ğŸ¯ æ–¹å‘å‡†ç¡®ç‡: {directional_accuracy:.1f}%")
        
        return {
            "success": True,
            "predictions": predictions,
            "directions": directions,
            "quality_metrics": quality_metrics,
            "model_info": {
                "model_type": "Enhanced BiLSTM + Attention",
                "device": self.device_name,
                "sequence_length": self.sequence_length,
                "feature_size": cv_splits[0]['X_train'].shape[2],
                "training_samples": len(cv_splits[-1]['X_train']),
                "test_samples": len(cv_splits[-1]['X_test']),
                "cv_folds": len(cv_splits)
            },
            "training_history": {
                "train_losses": train_losses,
                "val_losses": val_losses
            }
        }

# Streamlit WebUI
def main():
    """ä¸»WebUIåº”ç”¨"""
    st.set_page_config(
        page_title="å¢å¼ºLSTMä»·æ ¼é¢„æµ‹ç³»ç»Ÿ v2.0",
        page_icon="ğŸš€",
        layout="wide"
    )
    
    st.title("ğŸš€ å¢å¼ºLSTMä»·æ ¼é¢„æµ‹ç³»ç»Ÿ v2.0")
    st.markdown("**åŒå‘LSTM + Attention + æŠ€æœ¯æŒ‡æ ‡ + æ»šåŠ¨äº¤å‰éªŒè¯çš„æ·±åº¦å­¦ä¹ é¢„æµ‹**")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ é¢„æµ‹é…ç½®")
        
        symbol = st.selectbox("é€‰æ‹©äº¤æ˜“å¯¹", [
            "DOGEUSDT", "BTCUSDT", "ETHUSDT", "ADAUSDT", 
            "DOTUSDT", "LINKUSDT", "LTCUSDT", "BCHUSDT",
            "XLMUSDT", "EOSUSDT", "MATICUSDT"
        ])
        
        sequence_length = st.slider("LSTMåºåˆ—é•¿åº¦", 20, 60, 30)
        prediction_hours = st.slider("é¢„æµ‹æ—¶é•¿ (å°æ—¶)", 1, 168, 24)
        
        if st.button("ğŸš€ å¼€å§‹å¢å¼ºé¢„æµ‹", type="primary"):
            st.session_state.run_prediction = True
    
    # ä¸»ç•Œé¢
    if st.session_state.get('run_prediction', False):
        run_enhanced_prediction(symbol, sequence_length, prediction_hours)

def run_enhanced_prediction(symbol, sequence_length, prediction_hours):
    """è¿è¡Œå¢å¼ºLSTMé¢„æµ‹"""
    
    # æ˜¾ç¤ºæ•°æ®è·å–è¿›åº¦
    progress_container = st.container()
    with progress_container:
        st.info(f"ğŸ“Š æ­£åœ¨è·å– {symbol} çš„å¢å¼ºå†å²æ•°æ®...")
        progress_bar = st.progress(0)
        status_text = st.empty()
    
    try:
        # è·å–çœŸå®å†å²æ•°æ®
        status_text.text("ğŸ” ä»å¤šæ•°æ®æºè·å–15åˆ†é’Ÿæ•°æ®...")
        progress_bar.progress(20)
        
        fetcher = MultiSourceDataFetcher(symbol)
        source, historical_data = fetcher.fetch_large_dataset(target_points=2000)
        
        if historical_data is None or len(historical_data) < 100:
            st.error("âŒ æ— æ³•è·å–è¶³å¤Ÿçš„å†å²æ•°æ®ï¼Œè¯·ç¨åé‡è¯•æˆ–æ›´æ¢äº¤æ˜“å¯¹")
            return
        
        progress_bar.progress(40)
        status_text.text(f"âœ… æ•°æ®è·å–æˆåŠŸ: {len(historical_data)}æ¡è®°å½•")
        
        # æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
        time_span = (historical_data.index[-1] - historical_data.index[0]).total_seconds() / 86400
        current_price = historical_data['close'].iloc[-1]
        
        # è®¾å¤‡ä¿¡æ¯
        device, device_name = setup_optimized_device()
        
        with st.expander("ğŸ“Š æ•°æ®ä¸è®¾å¤‡ä¿¡æ¯"):
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ•°æ®é‡", f"{len(historical_data)}æ¡")
            with col2:
                st.metric("æ—¶é—´è·¨åº¦", f"{time_span:.1f}å¤©")
            with col3:
                st.metric("å½“å‰ä»·æ ¼", f"{current_price:.6f}")
            with col4:
                st.metric("è¿è¡Œè®¾å¤‡", device_name)
        
        # è¿è¡Œå¢å¼ºLSTMé¢„æµ‹
        status_text.text("ğŸ§  æ­£åœ¨è®­ç»ƒå¢å¼ºLSTMæ¨¡å‹...")
        progress_bar.progress(60)
        
        with st.spinner("æ­£åœ¨è®­ç»ƒå¢å¼ºLSTMæ¨¡å‹ï¼Œè¯·ç¨å€™..."):
            predictor = OptimizedLSTMSystem(symbol, sequence_length)
            result = predictor.enhanced_predict_sequence(historical_data, prediction_hours)
            
            progress_bar.progress(100)
            status_text.text("âœ… å¢å¼ºLSTMé¢„æµ‹å®Œæˆ!")
            
            if result and result["success"]:
                st.success("ğŸ‰ å¢å¼ºLSTMé¢„æµ‹ç”ŸæˆæˆåŠŸ!")
                
                # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                predictions = result["predictions"]
                directions = result["directions"]
                metrics = result["quality_metrics"]
                model_info = result["model_info"]
                
                # æ ¸å¿ƒæŒ‡æ ‡ï¼ˆå¢åŠ æ–°æŒ‡æ ‡ï¼‰
                col1, col2, col3, col4, col5 = st.columns(5)
                with col1:
                    st.metric("å½“å‰ä»·æ ¼", f"{metrics['current_price']:.6f}")
                with col2:
                    st.metric("é¢„æµ‹ä»·æ ¼", f"{metrics['predicted_price']:.6f}")
                with col3:
                    st.metric("å˜åŒ–å¹…åº¦", f"{metrics['change_pct']:+.2f}%")
                with col4:
                    st.metric("é¢„æµ‹è¶‹åŠ¿", metrics['trend'])
                with col5:
                    st.metric("æ–¹å‘å‡†ç¡®ç‡", f"{metrics['directional_accuracy']:.1f}%")
                
                # ç»˜åˆ¶é¢„æµ‹å›¾è¡¨
                fig = go.Figure()
                
                # å†å²ä»·æ ¼ (æœ€è¿‘100ä¸ªç‚¹)
                recent_data = historical_data.tail(100)
                fig.add_trace(go.Scatter(
                    x=recent_data.index,
                    y=recent_data['close'],
                    name="å†å²ä»·æ ¼",
                    line=dict(color='#1f77b4', width=2),
                    hovertemplate='æ—¶é—´: %{x}<br>ä»·æ ¼: %{y:.6f}<extra></extra>'
                ))
                
                # é¢„æµ‹ä»·æ ¼
                fig.add_trace(go.Scatter(
                    x=predictions.index,
                    y=predictions['close'],
                    name="Enhanced LSTMé¢„æµ‹",
                    line=dict(color='#ff7f0e', dash='dash', width=2),
                    hovertemplate='æ—¶é—´: %{x}<br>é¢„æµ‹ä»·æ ¼: %{y:.6f}<extra></extra>'
                ))
                
                fig.update_layout(
                    title=f"ğŸ”® {symbol} Enhanced LSTMä»·æ ¼é¢„æµ‹ (åŒå‘+Attention)",
                    xaxis_title="æ—¶é—´",
                    yaxis_title="ä»·æ ¼ (USDT)",
                    height=600,
                    showlegend=True,
                    hovermode='x unified'
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # æ¨¡å‹è¯¦æƒ…ï¼ˆå¢å¼ºç‰ˆï¼‰
                col1, col2 = st.columns(2)
                with col1:
                    st.subheader("ğŸ“ˆ å¢å¼ºé¢„æµ‹åˆ†æ")
                    st.write(f"**é¢„æµ‹è¶‹åŠ¿**: {metrics['trend']}")
                    st.write(f"**ç½®ä¿¡åº¦**: {metrics['confidence']:.1f}%")
                    st.write(f"**MSE**: {metrics['mse']:.8f}")
                    st.write(f"**MAE**: {metrics['mae']:.6f}")
                    st.write(f"**MAPE**: {metrics['mape']:.2f}%")
                    st.write(f"**æ–¹å‘å‡†ç¡®ç‡**: {metrics['directional_accuracy']:.1f}%")
                    
                    # æ–¹å‘é¢„æµ‹ä¿¡æ¯
                    st.write("---")
                    st.write(f"**ä¸»è¦æ–¹å‘**: {metrics['main_direction']}")
                    st.write(f"**æ–¹å‘ç½®ä¿¡åº¦**: {metrics['direction_confidence']:.2f}")
                
                with col2:
                    st.subheader("ğŸ¯ å¢å¼ºæ¨¡å‹ä¿¡æ¯")
                    st.write(f"**æ¨¡å‹ç±»å‹**: {model_info['model_type']}")
                    st.write(f"**è¿è¡Œè®¾å¤‡**: {model_info['device']}")
                    st.write(f"**åºåˆ—é•¿åº¦**: {model_info['sequence_length']}")
                    st.write(f"**ç‰¹å¾ç»´åº¦**: {model_info['feature_size']}")
                    st.write(f"**è®­ç»ƒæ ·æœ¬**: {model_info['training_samples']}")
                    st.write(f"**äº¤å‰éªŒè¯æŠ˜æ•°**: {model_info['cv_folds']}")
                
                # æ–¹å‘é¢„æµ‹è¯¦æƒ…
                with st.expander("ğŸ¯ æ–¹å‘é¢„æµ‹è¯¦æƒ…"):
                    # åˆ›å»ºæ–¹å‘é¢„æµ‹å›¾è¡¨
                    fig_dir = go.Figure()
                    
                    # æ–¹å‘ç½®ä¿¡åº¦
                    colors = ['green' if d == 'ä¸Šæ¶¨' else 'red' for d in directions['direction']]
                    fig_dir.add_trace(go.Bar(
                        x=directions.index,
                        y=directions['confidence'],
                        name="æ–¹å‘ç½®ä¿¡åº¦",
                        marker_color=colors,
                        hovertemplate='æ—¶é—´: %{x}<br>æ–¹å‘: %{text}<br>ç½®ä¿¡åº¦: %{y:.2f}<extra></extra>',
                        text=directions['direction']
                    ))
                    
                    fig_dir.update_layout(
                        title="æ–¹å‘é¢„æµ‹ç½®ä¿¡åº¦",
                        xaxis_title="æ—¶é—´",
                        yaxis_title="ç½®ä¿¡åº¦",
                        height=400
                    )
                    st.plotly_chart(fig_dir, use_container_width=True)
                
                # è®­ç»ƒå†å²
                if "training_history" in result:
                    with st.expander("ğŸ“ˆ è®­ç»ƒå†å²"):
                        train_losses = result["training_history"]["train_losses"]
                        val_losses = result["training_history"]["val_losses"]
                        
                        fig_loss = go.Figure()
                        fig_loss.add_trace(go.Scatter(
                            y=train_losses,
                            name="è®­ç»ƒæŸå¤±",
                            line=dict(color='blue')
                        ))
                        fig_loss.add_trace(go.Scatter(
                            y=val_losses,
                            name="éªŒè¯æŸå¤±",
                            line=dict(color='red')
                        ))
                        fig_loss.update_layout(
                            title="Enhanced LSTMè®­ç»ƒæŸå¤±æ›²çº¿",
                            xaxis_title="è®­ç»ƒè½®æ¬¡",
                            yaxis_title="æŸå¤±å€¼",
                            height=400
                        )
                        st.plotly_chart(fig_loss, use_container_width=True)
                
                # é¢„æµ‹æ•°æ®è¡¨æ ¼ï¼ˆåŒ…å«æ–¹å‘ï¼‰
                with st.expander("ğŸ“‹ è¯¦ç»†é¢„æµ‹æ•°æ®"):
                    # åˆå¹¶ä»·æ ¼å’Œæ–¹å‘é¢„æµ‹
                    combined_pred = predictions.copy()
                    combined_pred['predicted_direction'] = directions['direction']
                    combined_pred['direction_confidence'] = directions['confidence'].round(3)
                    st.dataframe(combined_pred.round(6))
            else:
                st.error("âŒ å¢å¼ºLSTMé¢„æµ‹ç”Ÿæˆå¤±è´¥!")
                
    except Exception as e:
        st.error(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        st.text(traceback.format_exc())
        
    finally:
        # æ¸…ç†è¿›åº¦æ˜¾ç¤º
        progress_container.empty()
    
    st.session_state.run_prediction = False

if __name__ == "__main__":
    main() 