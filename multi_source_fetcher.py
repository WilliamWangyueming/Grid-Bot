"""
ä¼˜åŒ–ç‰ˆå¤šæ•°æ®æºè·å–å™¨ - æ™ºèƒ½æ•°æ®æºé€‰æ‹©ï¼Œé¿å…é‡å¤æ•°æ®
è·å–DOGEç­‰å¸ç§çš„15åˆ†é’Ÿäº¤æ˜“æ•°æ®ï¼Œæ”¯æŒä¸€å¹´å†å²æ•°æ®
"""
import requests
import pandas as pd
import time
from datetime import datetime, timedelta
import numpy as np
import zipfile
import io
import os
import yfinance as yf
import warnings
warnings.filterwarnings('ignore')

class EnhancedMultiSourceDataFetcher:
    """ä¼˜åŒ–ç‰ˆå¤šæ•°æ®æºè·å–å™¨ - æ™ºèƒ½æ•°æ®æºé€‰æ‹©"""
    
    def __init__(self, symbol="DOGEUSDT"):
        self.symbol = symbol
        # æ•°æ®æºæŒ‰ä¼˜å…ˆçº§æ’åºï¼šè´¨é‡ + æ•°æ®é‡ + ç¨³å®šæ€§
        self.data_sources = [
            "yahoo_finance",       # âœ… æœ€ä¼˜ï¼šçœŸå®DOGEæ•°æ®ï¼Œ1å¹´å†å²ï¼Œå¤šæ—¶é—´é¢—ç²’åº¦
            "huobi_api",           # âœ… é«˜è´¨é‡ï¼š15åˆ†é’Ÿæ•°æ®ï¼Œé«˜å¯†åº¦ï¼Œç¨³å®š
            "kraken_ohlc",         # âœ… ç¨³å®šï¼šå®˜æ–¹APIï¼Œä½†æ•°æ®é‡è¾ƒå°‘
        ]
        
    def fetch_all_sources(self, target_points=2000):
        """æ™ºèƒ½è·å–æ•°æ®ï¼šä¼˜å…ˆé€‰æ‹©æœ€ä½³æ•°æ®æºï¼Œé¿å…ä¸å¿…è¦çš„é‡å¤"""
        
        print("ğŸš€ å¯åŠ¨æ™ºèƒ½å¤šæ•°æ®æºè·å–")
        print("=" * 60)
        print(f"ğŸ¯ äº¤æ˜“å¯¹: {self.symbol}")
        print(f"ğŸ“Š ç›®æ ‡æ•°æ®é‡: {target_points} æ¡")
        print(f"ğŸ§  æ™ºèƒ½ç­–ç•¥: ä¼˜å…ˆçº§é€‰æ‹© + è´¨é‡è¯„ä¼°")
        
        best_source = None
        best_data = None
        all_sources_data = []
        
        # 1. Yahoo Finance - æœ€é«˜ä¼˜å…ˆçº§ï¼ˆæ··åˆç­–ç•¥ï¼‰
        print(f"\nğŸ“Š 1. Yahoo Finance (æ™ºèƒ½æ··åˆç­–ç•¥)")
        print("-" * 50)
        try:
            data = self._fetch_yahoo_finance_smart()
            if data is not None and len(data) > 100:
                time_span = (data.index[-1] - data.index[0]).total_seconds() / 86400
                data_density = len(data) / time_span
                
                print(f"âœ… Yahoo Finance æˆåŠŸ")
                print(f"   ğŸ“ˆ æ•°æ®é‡: {len(data)}æ¡")
                print(f"   â° æ—¶é—´è·¨åº¦: {time_span:.1f}å¤©")
                print(f"   ğŸ“Š æ•°æ®å¯†åº¦: {data_density:.1f} æ¡/å¤©")
                
                # è¯„ä¼°Yahooæ•°æ®è´¨é‡
                quality_score = self._evaluate_data_quality(data, time_span)
                print(f"   ğŸ† è´¨é‡è¯„åˆ†: {quality_score:.1f}/10")
                
                all_sources_data.append(("yahoo_finance", data, quality_score))
                
                # å¦‚æœYahooæ•°æ®å·²ç»å¾ˆå¥½ï¼Œå¯èƒ½ä¸éœ€è¦å…¶ä»–æ•°æ®æº
                if len(data) >= target_points and time_span >= 30:  # è‡³å°‘1ä¸ªæœˆæ•°æ®
                    print(f"   ğŸ¯ Yahooæ•°æ®å·²æ»¡è¶³éœ€æ±‚ï¼Œè·³è¿‡å…¶ä»–æ•°æ®æº")
                    best_source = "yahoo_finance"
                    best_data = data
                    return self._finalize_result(best_source, best_data)
                    
        except Exception as e:
            print(f"âŒ Yahoo Finance é”™è¯¯: {str(e)}")
        
        # 2. Huobi API - å¦‚æœYahooä¸å¤Ÿå¥½æ‰ä½¿ç”¨
        print(f"\nğŸ“Š 2. Huobi API (15åˆ†é’Ÿé«˜å¯†åº¦æ•°æ®)")
        print("-" * 50)
        try:
            data = self._fetch_huobi_15min()
            if data is not None and len(data) > 100:
                time_span = (data.index[-1] - data.index[0]).total_seconds() / 86400
                data_density = len(data) / time_span
                
                print(f"âœ… Huobi API æˆåŠŸ")
                print(f"   ğŸ“ˆ æ•°æ®é‡: {len(data)}æ¡")
                print(f"   â° æ—¶é—´è·¨åº¦: {time_span:.1f}å¤©")
                print(f"   ğŸ“Š æ•°æ®å¯†åº¦: {data_density:.1f} æ¡/å¤©")
                
                quality_score = self._evaluate_data_quality(data, time_span)
                print(f"   ğŸ† è´¨é‡è¯„åˆ†: {quality_score:.1f}/10")
                
                all_sources_data.append(("huobi_api", data, quality_score))
                
        except Exception as e:
            print(f"âŒ Huobi API é”™è¯¯: {str(e)}")
        
        # 3. Kraken OHLC - ä½œä¸ºè¡¥å……
        print(f"\nğŸ“Š 3. Kraken OHLC API (ç¨³å®šè¡¥å……æ•°æ®)")
        print("-" * 50)
        try:
            data = self._fetch_kraken_ohlc()
            if data is not None and len(data) > 100:
                time_span = (data.index[-1] - data.index[0]).total_seconds() / 86400
                data_density = len(data) / time_span
                
                print(f"âœ… Kraken OHLC æˆåŠŸ")
                print(f"   ğŸ“ˆ æ•°æ®é‡: {len(data)}æ¡")
                print(f"   â° æ—¶é—´è·¨åº¦: {time_span:.1f}å¤©")
                print(f"   ğŸ“Š æ•°æ®å¯†åº¦: {data_density:.1f} æ¡/å¤©")
                
                quality_score = self._evaluate_data_quality(data, time_span)
                print(f"   ğŸ† è´¨é‡è¯„åˆ†: {quality_score:.1f}/10")
                
                all_sources_data.append(("kraken_ohlc", data, quality_score))
                
        except Exception as e:
            print(f"âŒ Kraken OHLC é”™è¯¯: {str(e)}")
        
        # æ™ºèƒ½é€‰æ‹©æœ€ä½³æ•°æ®æº
        if all_sources_data:
            print(f"\nğŸ§  æ™ºèƒ½æ•°æ®æºé€‰æ‹©")
            print("=" * 60)
            
            # æŒ‰è´¨é‡è¯„åˆ†æ’åº
            all_sources_data.sort(key=lambda x: x[2], reverse=True)
            
            for i, (source_name, data, score) in enumerate(all_sources_data, 1):
                time_span = (data.index[-1] - data.index[0]).total_seconds() / 86400
                print(f"{i}. {source_name}: {len(data)}æ¡, {time_span:.1f}å¤©, è¯„åˆ†{score:.1f}")
            
            # é€‰æ‹©æœ€ä½³æ•°æ®æº
            best_source, best_data, best_score = all_sources_data[0]
            
            print(f"\nğŸ† é€‰æ‹©æœ€ä½³æ•°æ®æº: {best_source}")
            print(f"ğŸ“Š æœ€ç»ˆæ•°æ®: {len(best_data)}æ¡, è¯„åˆ†{best_score:.1f}")
            
            # æ™ºèƒ½åˆå¹¶ç­–ç•¥ï¼šåªæœ‰åœ¨æœ‰æ˜æ˜¾è¡¥å……ä»·å€¼æ—¶æ‰åˆå¹¶
            if len(all_sources_data) > 1:
                best_time_span = (best_data.index[-1] - best_data.index[0]).total_seconds() / 86400
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå¹¶å…¶ä»–æ•°æ®æº
                should_merge = False
                for source_name, data, score in all_sources_data[1:]:
                    time_span = (data.index[-1] - data.index[0]).total_seconds() / 86400
                    
                    # åªæœ‰åœ¨èƒ½æ˜¾è‘—å¢åŠ æ—¶é—´è·¨åº¦æˆ–æ•°æ®é‡æ—¶æ‰åˆå¹¶
                    if (time_span > best_time_span * 1.2 or  # æ—¶é—´è·¨åº¦å¢åŠ 20%ä»¥ä¸Š
                        len(data) > len(best_data) * 0.5):   # æ•°æ®é‡æ˜¯æœ€ä½³æºçš„50%ä»¥ä¸Š
                        should_merge = True
                        break
                
                if should_merge:
                    print(f"\nğŸ”„ æ‰§è¡Œæ™ºèƒ½æ•°æ®åˆå¹¶...")
                    combined_data = self._smart_merge_sources(all_sources_data)
                    if combined_data is not None and len(combined_data) > len(best_data) * 1.1:
                        improvement = len(combined_data) - len(best_data)
                        print(f"âœ… åˆå¹¶æˆåŠŸ: +{improvement}æ¡æ•°æ®")
                        best_source = "merged_smart"
                        best_data = combined_data
                    else:
                        print(f"âš ï¸ åˆå¹¶æ”¶ç›Šä¸æ˜æ˜¾ï¼Œä¿æŒå•ä¸€æ•°æ®æº")
                else:
                    print(f"âš ï¸ å…¶ä»–æ•°æ®æºæ— æ˜¾è‘—è¡¥å……ä»·å€¼ï¼Œä¿æŒæœ€ä½³å•ä¸€æ•°æ®æº")
            
            # ç»Ÿä¸€å°†Yahooæœ€ä½³æ•°æ®é‡é‡‡æ ·è‡³ä¸¥æ ¼15åˆ†é’Ÿæ—¶é—´è½´ï¼Œé¿å…åç»­é‡å¤å¡«å……
            best_data = self._resample_to_15m(best_data)
            return self._finalize_result(best_source, best_data)
        
        else:
            print("âŒ æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥äº†")
            return None, None
    
    def _evaluate_data_quality(self, data, time_span_days):
        """è¯„ä¼°æ•°æ®è´¨é‡ (0-10åˆ†)"""
        try:
            score = 0.0
            
            # 1. æ•°æ®é‡è¯„åˆ† (0-3åˆ†)
            if len(data) >= 4000:
                score += 3.0
            elif len(data) >= 2000:
                score += 2.5
            elif len(data) >= 1000:
                score += 2.0
            elif len(data) >= 500:
                score += 1.5
            else:
                score += len(data) / 500.0
            
            # 2. æ—¶é—´è·¨åº¦è¯„åˆ† (0-3åˆ†)
            if time_span_days >= 300:  # æ¥è¿‘1å¹´
                score += 3.0
            elif time_span_days >= 180:  # åŠå¹´
                score += 2.5
            elif time_span_days >= 90:   # 3ä¸ªæœˆ
                score += 2.0
            elif time_span_days >= 30:   # 1ä¸ªæœˆ
                score += 1.5
            else:
                score += time_span_days / 30.0
            
            # 3. æ•°æ®å¯†åº¦è¯„åˆ† (0-2åˆ†)
            density = len(data) / time_span_days
            if density >= 96:    # 15åˆ†é’Ÿçº§åˆ« (96æ¡/å¤©)
                score += 2.0
            elif density >= 24:  # å°æ—¶çº§åˆ« (24æ¡/å¤©)
                score += 1.5
            elif density >= 4:   # 6å°æ—¶çº§åˆ«
                score += 1.0
            else:
                score += density / 24.0
            
            # 4. æ•°æ®å®Œæ•´æ€§è¯„åˆ† (0-2åˆ†)
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±å€¼
            missing_ratio = data.isnull().sum().sum() / (len(data) * len(data.columns))
            if missing_ratio == 0:
                score += 2.0
            elif missing_ratio < 0.01:
                score += 1.5
            elif missing_ratio < 0.05:
                score += 1.0
            else:
                score += max(0, 1.0 - missing_ratio)
            
            return min(10.0, score)
            
        except:
            return 5.0  # é»˜è®¤ä¸­ç­‰è¯„åˆ†
    
    def _smart_merge_sources(self, all_sources_data):
        """
        æ™ºèƒ½åˆå¹¶å¤šæ•°æ®æºï¼Œç»Ÿä¸€é‡é‡‡æ ·åˆ°15åˆ†é’Ÿæ—¶é—´è½´
        """
        try:
            print("ğŸ”„ å¼€å§‹æ™ºèƒ½æ•°æ®æºåˆå¹¶...")
            
            all_data = []
            
            # é¢„å¤„ç†æ¯ä¸ªæ•°æ®æº
            for source_name, data, score in all_sources_data:
                # ç¡®ä¿æ•°æ®æœ‰æ—¶é—´ç´¢å¼•
                if not isinstance(data.index, pd.DatetimeIndex):
                    data.index = pd.to_datetime(data.index)
                
                # åˆ é™¤é‡å¤æ—¶é—´æˆ³ï¼Œä¿ç•™æœ€åä¸€ä¸ª
                data_clean = data[~data.index.duplicated(keep='last')]
                
                # ç»Ÿä¸€é‡é‡‡æ ·åˆ°15åˆ†é’Ÿ
                try:
                    resampled = data_clean.resample('15T').agg({
                        'open': 'first',
                        'high': 'max',
                        'low': 'min', 
                        'close': 'last',
                        'volume': 'sum'
                    })
                    
                    # å‰å‘å¡«å……ç¼ºå¤±å€¼
                    resampled = resampled.fillna(method='ffill')
                    
                    # åˆ é™¤NaNè¡Œ
                    resampled = resampled.dropna()
                    
                    if len(resampled) > 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                        all_data.append((source_name, resampled, score))
                        print(f"   âœ… {source_name}: é‡é‡‡æ ·åˆ° {len(resampled)} æ¡15åˆ†é’Ÿæ•°æ®")
                    else:
                        print(f"   âš ï¸ {source_name}: é‡é‡‡æ ·åæ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
                        
                except Exception as e:
                    print(f"   âŒ {source_name}: é‡é‡‡æ ·å¤±è´¥ - {str(e)}")
                    continue
            
            if len(all_data) < 2:
                print("   âš ï¸ å¯ç”¨æ•°æ®æºä¸è¶³ï¼Œæ— æ³•åˆå¹¶")
                return None
            
            # æ‰¾åˆ°æ—¶é—´èŒƒå›´çš„äº¤é›†å’Œå¹¶é›†
            earliest_start = min([data.index[0] for _, data, _ in all_data])
            latest_end = max([data.index[-1] for _, data, _ in all_data])
            
            print(f"   ğŸ“… åˆå¹¶æ—¶é—´èŒƒå›´: {earliest_start} åˆ° {latest_end}")
            
            # åˆ›å»ºç»Ÿä¸€çš„15åˆ†é’Ÿæ—¶é—´è½´
            unified_timeline = pd.date_range(
                start=earliest_start, 
                end=latest_end, 
                freq='15T'
            )
            
            # åŸºäºè´¨é‡è¯„åˆ†æ’åºï¼Œä¼˜å…ˆä½¿ç”¨é«˜è´¨é‡æ•°æ®
            all_data.sort(key=lambda x: x[2], reverse=True)
            
            # ä»æœ€é«˜è´¨é‡æ•°æ®æºå¼€å§‹
            primary_source, primary_data, primary_score = all_data[0]
            print(f"   ğŸ† ä¸»è¦æ•°æ®æº: {primary_source} (è¯„åˆ†: {primary_score:.1f})")
            
            # é‡å»ºç´¢å¼•åˆ°ç»Ÿä¸€æ—¶é—´è½´
            merged_data = primary_data.reindex(unified_timeline, method='nearest', tolerance='30T')
            
            # ç”¨å…¶ä»–æ•°æ®æºå¡«è¡¥ç¼ºå¤±
            for source_name, data, score in all_data[1:]:
                print(f"   ğŸ”— åˆå¹¶ {source_name} (è¯„åˆ†: {score:.1f})")
                
                # æ‰¾åˆ°ç¼ºå¤±çš„æ—¶é—´ç‚¹
                missing_mask = merged_data.isnull().any(axis=1)
                missing_times = merged_data[missing_mask].index
                
                if len(missing_times) > 0:
                    # ç”¨å½“å‰æ•°æ®æºå¡«è¡¥
                    fill_data = data.reindex(missing_times, method='nearest', tolerance='30T')
                    
                    # åªå¡«è¡¥æœ‰æ•ˆæ•°æ®
                    valid_fill = fill_data.dropna()
                    if len(valid_fill) > 0:
                        merged_data.loc[valid_fill.index] = valid_fill
                        filled_count = len(valid_fill)
                        print(f"     â†ªï¸ å¡«è¡¥äº† {filled_count} ä¸ªç¼ºå¤±æ—¶é—´ç‚¹")
                    else:
                        print(f"     âš ï¸ æ— æœ‰æ•ˆæ•°æ®å¯å¡«è¡¥")
                else:
                    print(f"     âœ… æ— éœ€å¡«è¡¥")
            
            # æœ€ç»ˆæ¸…ç†
            merged_data = merged_data.dropna()
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            if len(merged_data) > 0:
                # ç¡®ä¿ä»·æ ¼é€»è¾‘æ€§
                merged_data['high'] = merged_data[['open', 'high', 'close']].max(axis=1)
                merged_data['low'] = merged_data[['open', 'low', 'close']].min(axis=1)
                merged_data['volume'] = merged_data['volume'].clip(lower=0)  # æˆäº¤é‡éè´Ÿ
                
                time_span = (merged_data.index[-1] - merged_data.index[0]).total_seconds() / 86400
                data_density = len(merged_data) / time_span if time_span > 0 else 0
                
                print(f"   âœ… åˆå¹¶å®Œæˆ:")
                print(f"     ğŸ“Š æœ€ç»ˆæ•°æ®é‡: {len(merged_data)} æ¡")
                print(f"     â° æ—¶é—´è·¨åº¦: {time_span:.1f} å¤©")
                print(f"     ğŸ“ˆ æ•°æ®å¯†åº¦: {data_density:.1f} æ¡/å¤©")
                
                return merged_data
            else:
                print("   âŒ åˆå¹¶åæ— æœ‰æ•ˆæ•°æ®")
                return None
                
        except Exception as e:
            print(f"âŒ æ™ºèƒ½åˆå¹¶å¤±è´¥: {str(e)}")
            return None
    
    def _finalize_result(self, source, data):
        """æœ€ç»ˆç»“æœå¤„ç†å’Œå±•ç¤º"""
        if data is None:
            return None, None
        
        time_span_days = (data.index[-1] - data.index[0]).total_seconds()/86400
        time_span_years = time_span_days / 365
        
        print(f"\nğŸ‰ æ•°æ®è·å–å®Œæˆ")
        print("=" * 60)
        print(f"ğŸ† æœ€ç»ˆæ•°æ®æº: {source}")
        print(f"ğŸ“ˆ æ•°æ®é‡: {len(data):,} æ¡")
        print(f"â° æ—¶é—´è·¨åº¦: {time_span_days:.1f}å¤© ({time_span_years:.2f}å¹´)")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")
        print(f"ğŸ’° ä»·æ ¼èŒƒå›´: {data['close'].min():.6f} - {data['close'].max():.6f}")
        print(f"ğŸ“Š æ•°æ®å¯†åº¦: {len(data)/time_span_days:.1f} æ¡/å¤©")
        
        # æ•°æ®è´¨é‡è¯„ä¼°
        quality_score = self._evaluate_data_quality(data, time_span_days)
        if quality_score >= 8.5:
            print(f"ğŸ† æ•°æ®è´¨é‡: å“è¶Š ({quality_score:.1f}/10)")
        elif quality_score >= 7.0:
            print(f"ğŸ¥‡ æ•°æ®è´¨é‡: ä¼˜ç§€ ({quality_score:.1f}/10)")
        elif quality_score >= 5.5:
            print(f"ğŸ‘ æ•°æ®è´¨é‡: è‰¯å¥½ ({quality_score:.1f}/10)")
        else:
            print(f"âš ï¸ æ•°æ®è´¨é‡: ä¸€èˆ¬ ({quality_score:.1f}/10)")
        
        return source, data
    
    def _fetch_yahoo_finance_smart(self):
        """æ™ºèƒ½è·å– Yahoo Finance æ•°æ® - ä¼˜å…ˆ15åˆ†é’Ÿï¼Œ1å°æ—¶è¡¥å……å†å²"""
        try:
            print("   ğŸ§  æ™ºèƒ½ Yahoo Finance ç­–ç•¥...")
            
            symbol = "DOGE-USD"
            ticker = yf.Ticker(symbol)
            print(f"   ğŸ“¥ è·å–ç¬¦å·: {symbol}")
            
            # ç­–ç•¥é€‰æ‹©ï¼šæ ¹æ®éœ€æ±‚æ™ºèƒ½é€‰æ‹©æœ€ä½³ç­–ç•¥
            strategies = []
            
            # ç­–ç•¥1: æœ€å¤§èŒƒå›´15åˆ†é’Ÿæ•°æ®ï¼ˆä¼˜å…ˆç­–ç•¥ï¼‰
            print("   ğŸ“Š ç­–ç•¥1: æœ€å¤§èŒƒå›´15åˆ†é’Ÿæ•°æ®...")
            df_15m_max = None
            for period in ["60d", "30d", "7d"]:  # å°è¯•ä¸åŒæ—¶é—´èŒƒå›´
                try:
                    print(f"      ğŸ” å°è¯•15åˆ†é’Ÿæ•°æ® ({period})...")
                    data_15m = ticker.history(period=period, interval="15m", auto_adjust=False, prepost=False)
                    if len(data_15m) > 100:
                        df_15m_temp = self._convert_yahoo_data(data_15m)
                        time_span = (df_15m_temp.index[-1] - df_15m_temp.index[0]).total_seconds() / 86400
                        print(f"         âœ… 15åˆ†é’Ÿæ•°æ® ({period}): {len(df_15m_temp)}æ¡, {time_span:.1f}å¤©")
                        
                        # é€‰æ‹©æ•°æ®æœ€å¤šçš„15åˆ†é’Ÿæ•°æ®é›†
                        if df_15m_max is None or len(df_15m_temp) > len(df_15m_max):
                            df_15m_max = df_15m_temp
                            
                except Exception as e:
                    print(f"         âŒ 15åˆ†é’Ÿæ•°æ® ({period}) å¤±è´¥: {str(e)}")
            
            if df_15m_max is not None:
                time_span_15m = (df_15m_max.index[-1] - df_15m_max.index[0]).total_seconds() / 86400
                strategies.append(("15min_max", df_15m_max, len(df_15m_max), time_span_15m))
                print(f"      ğŸ† æœ€ä½³15åˆ†é’Ÿæ•°æ®: {len(df_15m_max)}æ¡, {time_span_15m:.1f}å¤©")
            
            # ç­–ç•¥2: 1å¹´1å°æ—¶æ•°æ®ï¼ˆè¡¥å……å†å²ç”¨ï¼‰
            print("   ğŸ“Š ç­–ç•¥2: 1å¹´1å°æ—¶å†å²æ•°æ®...")
            df_1h = None
            for period in ["1y", "max"]:  # å°è¯•è·å–æœ€é•¿å†å²
                try:
                    print(f"      ğŸ” å°è¯•1å°æ—¶æ•°æ® ({period})...")
                    data_1h = ticker.history(period=period, interval="1h", auto_adjust=False, prepost=False)
                    if len(data_1h) > 100:
                        df_1h_temp = self._convert_yahoo_data(data_1h)
                        time_span = (df_1h_temp.index[-1] - df_1h_temp.index[0]).total_seconds() / 86400
                        print(f"         âœ… 1å°æ—¶æ•°æ® ({period}): {len(df_1h_temp)}æ¡, {time_span:.1f}å¤©")
                        
                        # é€‰æ‹©æ—¶é—´è·¨åº¦æœ€é•¿çš„1å°æ—¶æ•°æ®
                        if df_1h is None or time_span > (df_1h.index[-1] - df_1h.index[0]).total_seconds() / 86400:
                            df_1h = df_1h_temp
                            
                except Exception as e:
                    print(f"         âŒ 1å°æ—¶æ•°æ® ({period}) å¤±è´¥: {str(e)}")
            
            if df_1h is not None:
                time_span_1h = (df_1h.index[-1] - df_1h.index[0]).total_seconds() / 86400
                strategies.append(("1hour_max", df_1h, len(df_1h), time_span_1h))
                print(f"      ğŸ† æœ€ä½³1å°æ—¶æ•°æ®: {len(df_1h)}æ¡, {time_span_1h:.1f}å¤©")
            
            # ç­–ç•¥3: æ™ºèƒ½èåˆ - 15åˆ†é’Ÿä¼˜å…ˆ + 1å°æ—¶è¡¥å……å†å²
            if df_15m_max is not None and df_1h is not None:
                print("   ğŸ“Š ç­–ç•¥3: æ™ºèƒ½èåˆ (15åˆ†é’Ÿä¼˜å…ˆ + 1å°æ—¶è¡¥å……)...")
                try:
                    # åˆ†æ15åˆ†é’Ÿæ•°æ®çš„æ—¶é—´èŒƒå›´
                    min_15m = df_15m_max.index.min()
                    max_15m = df_15m_max.index.max()
                    
                    print(f"      ğŸ“… 15åˆ†é’Ÿæ•°æ®èŒƒå›´: {min_15m} åˆ° {max_15m}")
                    
                    # ä»1å°æ—¶æ•°æ®ä¸­æå–15åˆ†é’Ÿæ•°æ®èŒƒå›´ä¹‹å¤–çš„å†å²æ•°æ®
                    df_1h_historical = df_1h[df_1h.index < min_15m]
                    df_1h_future = df_1h[df_1h.index > max_15m]  # ä¹Ÿå¯èƒ½æœ‰æœªæ¥æ•°æ®
                    
                    print(f"      ğŸ“Š 1å°æ—¶å†å²æ•°æ®: {len(df_1h_historical)}æ¡")
                    print(f"      ğŸ“Š 1å°æ—¶æœªæ¥æ•°æ®: {len(df_1h_future)}æ¡")
                    
                    # æ™ºèƒ½èåˆç­–ç•¥
                    fusion_parts = []
                    
                    # 1. æ·»åŠ 1å°æ—¶å†å²æ•°æ®ï¼ˆ15åˆ†é’Ÿæ•°æ®ä¹‹å‰çš„æ—¶é—´æ®µï¼‰
                    if len(df_1h_historical) > 0:
                        fusion_parts.append(df_1h_historical)
                        print(f"      âœ… æ·»åŠ å†å²1å°æ—¶æ•°æ®: {len(df_1h_historical)}æ¡")
                    
                    # 2. æ·»åŠ 15åˆ†é’Ÿé«˜ç²¾åº¦æ•°æ®ï¼ˆä¸»è¦æ•°æ®ï¼‰
                    fusion_parts.append(df_15m_max)
                    print(f"      âœ… æ·»åŠ 15åˆ†é’Ÿé«˜ç²¾åº¦æ•°æ®: {len(df_15m_max)}æ¡")
                    
                    # 3. æ·»åŠ 1å°æ—¶æœªæ¥æ•°æ®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    if len(df_1h_future) > 0:
                        fusion_parts.append(df_1h_future)
                        print(f"      âœ… æ·»åŠ æœªæ¥1å°æ—¶æ•°æ®: {len(df_1h_future)}æ¡")
                    
                    # åˆå¹¶æ‰€æœ‰æ•°æ®
                    if len(fusion_parts) > 1:
                        combined_df = pd.concat(fusion_parts).sort_index()
                        # å»é™¤é‡å¤æ—¶é—´æˆ³ï¼ˆä¼˜å…ˆä¿ç•™15åˆ†é’Ÿæ•°æ®ï¼‰
                        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
                        
                        time_span_combined = (combined_df.index[-1] - combined_df.index[0]).total_seconds() / 86400
                        
                        # è®¡ç®—æ•°æ®æ„æˆ
                        historical_days = 0
                        if len(df_1h_historical) > 0:
                            historical_days = (df_1h_historical.index[-1] - df_1h_historical.index[0]).total_seconds() / 86400
                        
                        main_days = (df_15m_max.index[-1] - df_15m_max.index[0]).total_seconds() / 86400
                        
                        strategies.append(("fusion_optimal", combined_df, len(combined_df), time_span_combined))
                        
                        print(f"      ğŸ‰ èåˆæ•°æ®åˆ›å»ºæˆåŠŸ!")
                        print(f"         ğŸ“ˆ æ€»æ•°æ®é‡: {len(combined_df)}æ¡")
                        print(f"         â° æ€»æ—¶é—´è·¨åº¦: {time_span_combined:.1f}å¤© ({time_span_combined/365:.2f}å¹´)")
                        print(f"         ğŸ“Š å†å²1å°æ—¶æ®µ: {len(df_1h_historical)}æ¡ ({historical_days:.1f}å¤©)")
                        print(f"         ğŸ” é«˜ç²¾åº¦15åˆ†é’Ÿæ®µ: {len(df_15m_max)}æ¡ ({main_days:.1f}å¤©)")
                        if len(df_1h_future) > 0:
                            future_days = (df_1h_future.index[-1] - df_1h_future.index[0]).total_seconds() / 86400
                            print(f"         ğŸ“ˆ æœªæ¥1å°æ—¶æ®µ: {len(df_1h_future)}æ¡ ({future_days:.1f}å¤©)")
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°1å¹´ç›®æ ‡
                        if time_span_combined >= 300:  # çº¦10ä¸ªæœˆ
                            print(f"         ğŸ¯ å·²è¾¾åˆ°é•¿æœŸå†å²ç›®æ ‡ ({time_span_combined/365:.2f}å¹´)")
                        else:
                            print(f"         âš ï¸ æ—¶é—´è·¨åº¦æœªè¾¾åˆ°1å¹´ç›®æ ‡ï¼Œä½†å·²æœ€å¤§åŒ–åˆ©ç”¨å¯ç”¨æ•°æ®")
                    
                except Exception as e:
                    print(f"      âŒ èåˆç­–ç•¥å¤±è´¥: {str(e)}")
            
            # ç­–ç•¥4: å¦‚æœåªæœ‰15åˆ†é’Ÿæ•°æ®
            elif df_15m_max is not None:
                print("   ğŸ“Š ç­–ç•¥4: ä»…15åˆ†é’Ÿæ•°æ® (æ— 1å°æ—¶è¡¥å……)...")
                strategies.append(("15min_only", df_15m_max, len(df_15m_max), time_span_15m))
                print(f"      âš ï¸ ä»…æœ‰15åˆ†é’Ÿæ•°æ®ï¼Œæ— æ³•è·å–1å°æ—¶å†å²è¡¥å……")
            
            # ç­–ç•¥5: å¦‚æœåªæœ‰1å°æ—¶æ•°æ®  
            elif df_1h is not None:
                print("   ğŸ“Š ç­–ç•¥5: ä»…1å°æ—¶æ•°æ® (æ— 15åˆ†é’Ÿæ•°æ®)...")
                strategies.append(("1hour_only", df_1h, len(df_1h), time_span_1h))
                print(f"      âš ï¸ ä»…æœ‰1å°æ—¶æ•°æ®ï¼Œæ— æ³•è·å–15åˆ†é’Ÿé«˜ç²¾åº¦æ•°æ®")
            
            # é€‰æ‹©æœ€ä½³ç­–ç•¥ - ä¼˜å…ˆèåˆç­–ç•¥
            if strategies:
                print("   ğŸ§  è¯„ä¼°ç­–ç•¥ä¼˜åŠ£...")
                
                # è®¡ç®—æ¯ä¸ªç­–ç•¥çš„ç»¼åˆè¯„åˆ†ï¼Œèåˆç­–ç•¥é¢å¤–åŠ åˆ†
                scored_strategies = []
                for strategy_name, df, count, time_span in strategies:
                    base_score = self._evaluate_data_quality(df, time_span)
                    
                    # èåˆç­–ç•¥é¢å¤–å¥–åŠ±
                    if "fusion" in strategy_name:
                        bonus_score = 1.0  # èåˆç­–ç•¥é¢å¤–1åˆ†
                        print(f"      {strategy_name}: {count}æ¡, {time_span:.1f}å¤©, è¯„åˆ†{base_score:.1f}+{bonus_score:.1f}(èåˆå¥–åŠ±)={base_score+bonus_score:.1f}")
                        scored_strategies.append((strategy_name, df, base_score + bonus_score))
                    else:
                        scored_strategies.append((strategy_name, df, base_score))
                        print(f"      {strategy_name}: {count}æ¡, {time_span:.1f}å¤©, è¯„åˆ†{base_score:.1f}")
                
                # é€‰æ‹©æœ€é«˜è¯„åˆ†çš„ç­–ç•¥
                best_strategy = max(scored_strategies, key=lambda x: x[2])
                strategy_name, best_df, best_score = best_strategy
                
                print(f"   ğŸ† é€‰æ‹©ç­–ç•¥: {strategy_name} (è¯„åˆ†: {best_score:.1f})")
                # ç»Ÿä¸€å°†Yahooæœ€ä½³æ•°æ®é‡é‡‡æ ·è‡³ä¸¥æ ¼15åˆ†é’Ÿæ—¶é—´è½´ï¼Œé¿å…åç»­é‡å¤å¡«å……
                best_df = self._resample_to_15m(best_df)
                return best_df
            
            print("   âŒ æ‰€æœ‰Yahooç­–ç•¥éƒ½å¤±è´¥äº†")
            return None
            
        except Exception as e:
            print(f"   âŒ Yahoo Finance æ™ºèƒ½ç­–ç•¥å¤±è´¥: {str(e)}")
            return None
    
    def _convert_yahoo_data(self, data):
        """è½¬æ¢Yahoo Financeæ•°æ®æ ¼å¼"""
        df = pd.DataFrame()
        df['open'] = data['Open']
        df['high'] = data['High'] 
        df['low'] = data['Low']
        df['close'] = data['Close']
        df['volume'] = data['Volume']
        
        # ç»Ÿä¸€æ—¶åŒºå¤„ç†
        if data.index.tz is not None:
            df.index = data.index.tz_convert('UTC').tz_localize(None)
        else:
            df.index = pd.to_datetime(data.index)
        
        return df.sort_index()

    def _fetch_kraken_ohlc(self):
        """è·å– Kraken OHLC æ•°æ®"""
        try:
            print("   ğŸ”„ å°è¯• Kraken API...")
            
            # å°è¯•å¤šä¸ªKrakençš„DOGEäº¤æ˜“å¯¹
            pairs = ["DOGEUSD", "XDGUSD"]
            
            for pair in pairs:
                try:
                    print(f"   ğŸ“¡ å°è¯• Kraken {pair}...")
                    
                    url = "https://api.kraken.com/0/public/OHLC"
                    params = {
                        "pair": pair,
                        "interval": 15,  # 15åˆ†é’Ÿé—´éš”
                        "since": int((datetime.now() - timedelta(days=30)).timestamp())
                    }
                    
                    response = requests.get(url, params=params, timeout=30)
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        if data.get("error") == [] and data.get("result"):
                            # Kraken è¿”å›çš„keyå¯èƒ½ä¸åŒ
                            result_key = None
                            for key in data["result"].keys():
                                if key != "last":
                                    result_key = key
                                    break
                            
                            if result_key and len(data["result"][result_key]) > 10:
                                df = self._convert_kraken_format(data["result"][result_key])
                                if df is not None:
                                    print(f"   âœ… Kraken {pair} æˆåŠŸ: {len(df)}æ¡æ•°æ®")
                                    return df
                        else:
                            print(f"   âš ï¸ Kraken {pair} APIé”™è¯¯: {data.get('error', 'Unknown')}")
                    else:
                        print(f"   âš ï¸ Kraken {pair} HTTPé”™è¯¯: {response.status_code}")
                    
                except Exception as e:
                    print(f"   âš ï¸ Kraken {pair} å¤±è´¥: {str(e)}")
                    continue
            
            print(f"   âŒ Kraken æ‰€æœ‰äº¤æ˜“å¯¹éƒ½å¤±è´¥")
            return None
            
        except Exception as e:
            print(f"   âŒ Kraken æ•´ä½“å¤±è´¥: {str(e)}")
            return None

    def _convert_kraken_format(self, data):
        """è½¬æ¢ Kraken OHLC æ•°æ®æ ¼å¼"""
        try:
            # Kraken OHLCæ ¼å¼: [timestamp, open, high, low, close, vwap, volume, count]
            df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'vwap', 'volume', 'count'])
            
            # è½¬æ¢æ•°æ®ç±»å‹
            for col in ['open', 'high', 'low', 'close', 'volume']:
                df[col] = pd.to_numeric(df[col])
            
            # ç»Ÿä¸€æ—¶åŒºå¤„ç†
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
            df.set_index('timestamp', inplace=True)
            
            return df[['open', 'high', 'low', 'close', 'volume']].sort_index()
            
        except Exception as e:
            print(f"   âŒ Kraken æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
            return None

    def _fetch_huobi_15min(self):
        """è·å–Huobi 15åˆ†é’Ÿæ•°æ®"""
        try:
            symbol = self.symbol.replace("USDT", "usdt").lower()
            
            print("   ğŸ”„ è·å–Huobiæ•°æ®(æœ€ä¼˜15åˆ†é’Ÿ)...")
            
            # ç›´æ¥è·å–15åˆ†é’Ÿé«˜è´¨é‡æ•°æ®
            url = "https://api.huobi.pro/market/history/kline"
            params = {
                "symbol": symbol,
                "period": "15min",
                "size": 2000  # è·å–æœ€å¤§å¯èƒ½çš„æ•°æ®é‡
            }
            
            response = requests.get(url, params=params, timeout=15)
            response.raise_for_status()
            result = response.json()
            
            if result.get("status") == "ok" and result.get("data"):
                data = result["data"]
                
                if len(data) > 100:
                    df = self._convert_huobi_data(data)
                    
                    # æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
                    min_ts = min(item['id'] for item in data)
                    max_ts = max(item['id'] for item in data)
                    min_time = datetime.fromtimestamp(min_ts)
                    max_time = datetime.fromtimestamp(max_ts)
                    time_span = (max_ts - min_ts) / (24 * 3600)  # å¤©æ•°
                    
                    print(f"   âœ… Huobi 15åˆ†é’Ÿæ•°æ®æˆåŠŸ: {len(df)}æ¡")
                    print(f"      ğŸ“… æ—¶é—´èŒƒå›´: {min_time} åˆ° {max_time}")
                    print(f"      â° æ—¶é—´è·¨åº¦: {time_span:.1f}å¤©")
                    print(f"      ğŸ’° ä»·æ ¼èŒƒå›´: {df['close'].min():.6f} - {df['close'].max():.6f} USDT")
                    print(f"      ğŸ“Š æ•°æ®å¯†åº¦: {len(df)/time_span:.1f} æ¡/å¤©")
                    
                    return df
                else:
                    print(f"   âŒ Huobiæ•°æ®é‡ä¸è¶³: {len(data)}æ¡")
            else:
                print(f"   âŒ Huobi APIè¿”å›é”™è¯¯: {result.get('err-msg', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as e:
            print(f"   âŒ Huobiæ•°æ®è·å–å¤±è´¥: {str(e)}")
        
        return None
    
    def _convert_huobi_data(self, data):
        """è½¬æ¢Huobiæ•°æ®æ ¼å¼"""
        df = pd.DataFrame(data)
        
        # Huobiæ•°æ®æ ¼å¼: {id, open, close, low, high, amount, vol, count}
        df = df.rename(columns={
            'id': 'timestamp',
            'vol': 'volume'
        })
        
        for col in ['open', 'high', 'low', 'close', 'volume']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col])
        
        # ç»Ÿä¸€æ—¶åŒºå¤„ç† - å»é™¤æ—¶åŒºä¿¡æ¯
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
        df.set_index('timestamp', inplace=True)
        
        return df[['open', 'high', 'low', 'close', 'volume']].sort_index()

    def _resample_to_15m(self, data):
        """å°†æ•°æ®é‡é‡‡æ ·åˆ°ä¸¥æ ¼15åˆ†é’Ÿæ—¶é—´è½´"""
        try:
            print("ğŸ”„ å¼€å§‹é‡é‡‡æ ·åˆ°15åˆ†é’Ÿæ—¶é—´è½´...")
            
            # ç¡®ä¿æ•°æ®æœ‰æ—¶é—´ç´¢å¼•
            if not isinstance(data.index, pd.DatetimeIndex):
                data.index = pd.to_datetime(data.index)
            
            # åˆ é™¤é‡å¤æ—¶é—´æˆ³ï¼Œä¿ç•™æœ€åä¸€ä¸ª
            data_clean = data[~data.index.duplicated(keep='last')]
            
            # ç»Ÿä¸€é‡é‡‡æ ·åˆ°15åˆ†é’Ÿ
            try:
                resampled = data_clean.resample('15T').agg({
                    'open': 'first',
                    'high': 'max',
                    'low': 'min', 
                    'close': 'last',
                    'volume': 'sum'
                })
                
                # å‰å‘å¡«å……ç¼ºå¤±å€¼
                resampled = resampled.fillna(method='ffill')
                
                # åˆ é™¤NaNè¡Œ
                resampled = resampled.dropna()
                
                if len(resampled) > 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                    print(f"   âœ… é‡é‡‡æ ·åˆ°15åˆ†é’Ÿæ—¶é—´è½´æˆåŠŸ: {len(resampled)} æ¡15åˆ†é’Ÿæ•°æ®")
                    return resampled
                else:
                    print(f"   âš ï¸ é‡é‡‡æ ·åæ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
                    
            except Exception as e:
                print(f"   âŒ é‡é‡‡æ ·å¤±è´¥ - {str(e)}")
                return None
            
        except Exception as e:
            print(f"âŒ é‡é‡‡æ ·å¤±è´¥: {str(e)}")
            return None

def test_enhanced_fetcher():
    """æµ‹è¯•ä¼˜åŒ–ç‰ˆæ•°æ®è·å–å™¨"""
    print("ğŸ§ª æµ‹è¯•ä¼˜åŒ–ç‰ˆå¤šæ•°æ®æºè·å–å™¨")
    print("=" * 60)
    
    fetcher = EnhancedMultiSourceDataFetcher("DOGEUSDT")
    source, data = fetcher.fetch_all_sources(target_points=2000)
    
    if data is not None:
        print(f"\nğŸ‰ å¤šæ•°æ®æºè·å–æˆåŠŸï¼")
        print("=" * 60)
        
        time_span_days = (data.index[-1] - data.index[0]).total_seconds()/86400
        time_span_years = time_span_days / 365
        
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®è·å–ç»“æœ:")
        print(f"   ğŸ† æœ€ä½³æ•°æ®æº: {source}")
        print(f"   ğŸ“ˆ æ•°æ®é‡: {len(data):,}æ¡")
        print(f"   â° æ—¶é—´è·¨åº¦: {time_span_days:.1f}å¤© ({time_span_years:.2f}å¹´)")
        print(f"   ğŸ“… æ—¶é—´èŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")
        print(f"   ğŸ’° ä»·æ ¼èŒƒå›´: {data['close'].min():.6f} - {data['close'].max():.6f}")
        print(f"   ğŸ“Š å¹³å‡æ—¥å¯†åº¦: {len(data)/time_span_days:.1f} æ¡/å¤©")
        
        # æ•°æ®è´¨é‡è¯„ä¼°
        if len(data) >= 4000:
            print(f"   ğŸ† æ•°æ®è´¨é‡: å“è¶Š (4,000+æ¡æ•°æ®)")
        elif len(data) >= 2000:
            print(f"   ğŸ¥‡ æ•°æ®è´¨é‡: ä¼˜ç§€ (2,000+æ¡æ•°æ®)")
        elif len(data) >= 1000:
            print(f"   ğŸ‘ æ•°æ®è´¨é‡: è‰¯å¥½ (1,000+æ¡æ•°æ®)")
        else:
            print(f"   âš ï¸ æ•°æ®è´¨é‡: ä¸€èˆ¬ (å°‘äº1,000æ¡)")
            
        # æ•°æ®æ—¶é—´è·¨åº¦è¯„ä¼°
        if time_span_years >= 0.8:
            print(f"   ğŸ“… æ—¶é—´è·¨åº¦: ä¼˜ç§€ (æ¥è¿‘1å¹´æ•°æ®)")
        elif time_span_years >= 0.5:
            print(f"   ğŸ“… æ—¶é—´è·¨åº¦: è‰¯å¥½ (åŠå¹´ä»¥ä¸Šæ•°æ®)")
        else:
            print(f"   ğŸ“… æ—¶é—´è·¨åº¦: ä¸€èˆ¬ (å°‘äºåŠå¹´æ•°æ®)")
        
        return True
    else:
        print("âŒ æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥äº†")
        return False

# ä¿æŒå‘åå…¼å®¹æ€§
class MultiSourceDataFetcher(EnhancedMultiSourceDataFetcher):
    """å‘åå…¼å®¹çš„æ•°æ®è·å–å™¨"""
    
    def fetch_large_dataset(self, target_points=2000):
        """ä¿æŒå‘åå…¼å®¹çš„æ–¹æ³•ï¼Œç›´æ¥è°ƒç”¨æ–°çš„å¤šæºè·å–"""
        return self.fetch_all_sources(target_points)

def test_huobi_fetcher():
    """ä¿æŒå‘åå…¼å®¹çš„æµ‹è¯•å‡½æ•°"""
    return test_enhanced_fetcher()

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•ä¼˜åŒ–åçš„æ•°æ®æº...")
    print("ğŸ’¡ åªä¿ç•™çœŸæ­£æœ‰æ•ˆçš„æ•°æ®æº")
    print("ğŸ“Š ç›®æ ‡ï¼šè·å–DOGE 15åˆ†é’Ÿæ•°æ®ï¼Œæœ€å¥½ä¸€å¹´å†å²")
    print()
    
    test_enhanced_fetcher() 