"""
ä¼˜åŒ–ç‰ˆå¤šæ•°æ®æºè·å–å™¨ - åªä¿ç•™çœŸæ­£æœ‰æ•ˆçš„æ•°æ®æº
è·å–DOGEç­‰å¸ç§çš„15åˆ†é’Ÿäº¤æ˜“æ•°æ®ï¼Œæ”¯æŒä¸€å¹´å†å²æ•°æ®
"""
import requests
import pandas as pd
import time
from datetime import datetime, timedelta
import numpy as np
import zipfile
import io
import os
import yfinance as yf
import warnings
warnings.filterwarnings('ignore')

class EnhancedMultiSourceDataFetcher:
    """ä¼˜åŒ–ç‰ˆå¤šæ•°æ®æºè·å–å™¨ - åªä¿ç•™æœ‰æ•ˆæ•°æ®æº"""
    
    def __init__(self, symbol="DOGEUSDT"):
        self.symbol = symbol
        # åªä¿ç•™çœŸæ­£æœ‰æ•ˆçš„æ•°æ®æº
        self.data_sources = [
            "yahoo_finance",       # âœ… å·²éªŒè¯ï¼šçœŸå®DOGEæ•°æ®ï¼Œ1å¹´å†å²
            "kraken_ohlc",         # âœ… ç¨³å®šï¼šå®˜æ–¹APIï¼Œè‰¯å¥½æ•°æ®
            "huobi_api",           # âœ… å¯é ï¼š15åˆ†é’Ÿæ•°æ®ï¼Œé«˜å¯†åº¦
        ]
        
    def fetch_all_sources(self, target_points=2000):
        """å°è¯•æ‰€æœ‰æœ‰æ•ˆæ•°æ®æºï¼Œè·å–æœ€å¤§æ•°æ®é‡"""
        
        print("ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆå¤šæ•°æ®æºè·å–")
        print("=" * 60)
        print(f"ğŸ¯ äº¤æ˜“å¯¹: {self.symbol}")
        print(f"ğŸ“Š ç›®æ ‡æ•°æ®é‡: {target_points} æ¡")
        print(f"ğŸ”„ æœ‰æ•ˆæ•°æ®æº: {len(self.data_sources)} ä¸ª")
        
        all_data = []
        successful_sources = []
        
        # 1. å°è¯• Yahoo Finance (æ··åˆç­–ç•¥)
        print(f"\nğŸ“Š 1. Yahoo Finance (æ··åˆç­–ç•¥)")
        print("-" * 50)
        try:
            data = self._fetch_yahoo_finance()
            if data is not None and len(data) > 100:
                time_span = (data.index[-1] - data.index[0]).total_seconds() / 86400
                print(f"âœ… æˆåŠŸ: {len(data)}æ¡æ•°æ®, {time_span:.1f}å¤©è·¨åº¦")
                all_data.append(("yahoo_finance", data))
                successful_sources.append("Yahoo Finance")
            else:
                print(f"âŒ å¤±è´¥: æ•°æ®ä¸è¶³æˆ–æ— æ•°æ®")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {str(e)}")
            
        # 2. å°è¯• Kraken OHLC (ç¨³å®šAPI)
        print(f"\nğŸ“Š 2. Kraken OHLC API (ç¨³å®šæ•°æ®æº)")
        print("-" * 50)
        try:
            data = self._fetch_kraken_ohlc()
            if data is not None and len(data) > 100:
                time_span = (data.index[-1] - data.index[0]).total_seconds() / 86400
                print(f"âœ… æˆåŠŸ: {len(data)}æ¡æ•°æ®, {time_span:.1f}å¤©è·¨åº¦")
                all_data.append(("kraken_ohlc", data))
                successful_sources.append("Kraken OHLC")
            else:
                print(f"âŒ å¤±è´¥: æ•°æ®ä¸è¶³æˆ–æ— æ•°æ®")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {str(e)}")
            
        # 3. å°è¯• Huobi API (é«˜å¯†åº¦15åˆ†é’Ÿæ•°æ®)
        print(f"\nğŸ“Š 3. Huobi API (é«˜å¯†åº¦15åˆ†é’Ÿæ•°æ®)")
        print("-" * 50)
        try:
            data = self._fetch_huobi_15min()
            if data is not None and len(data) > 100:
                time_span = (data.index[-1] - data.index[0]).total_seconds() / 86400
                print(f"âœ… æˆåŠŸ: {len(data)}æ¡æ•°æ®, {time_span:.1f}å¤©è·¨åº¦")
                all_data.append(("huobi_api", data))
                successful_sources.append("Huobi API")
            else:
                print(f"âŒ å¤±è´¥: æ•°æ®ä¸è¶³æˆ–æ— æ•°æ®")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {str(e)}")
            
        # åˆå¹¶æ‰€æœ‰æˆåŠŸçš„æ•°æ®æº
        if all_data:
            print(f"\nğŸ‰ æ•°æ®è·å–æ€»ç»“")
            print("=" * 60)
            print(f"âœ… æˆåŠŸæ•°æ®æº: {len(successful_sources)} ä¸ª")
            print(f"ğŸ“‹ æˆåŠŸåˆ—è¡¨: {', '.join(successful_sources)}")
            
            # é€‰æ‹©æ•°æ®é‡æœ€å¤§çš„æº
            best_source, best_data = max(all_data, key=lambda x: len(x[1]))
            
            print(f"\nğŸ† æœ€ä½³æ•°æ®æº: {best_source}")
            print(f"ğŸ“Š æœ€ä½³æ•°æ®é‡: {len(best_data)} æ¡")
            
            # å°è¯•åˆå¹¶æ•°æ®ï¼ˆä¿®å¤æ—¶åŒºé—®é¢˜ï¼‰
            if len(all_data) > 1:
                print(f"\nğŸ”„ å°è¯•åˆå¹¶å¤šä¸ªæ•°æ®æº...")
                combined_data = self._merge_multiple_sources_fixed(all_data)
                if combined_data is not None and len(combined_data) > len(best_data):
                    print(f"âœ… æ•°æ®åˆå¹¶æˆåŠŸ: {len(combined_data)} æ¡ (å¢åŠ äº† {len(combined_data)-len(best_data)} æ¡)")
                    best_source = "merged_sources"
                    best_data = combined_data
                else:
                    print(f"âš ï¸ æ•°æ®åˆå¹¶åæ— æ˜æ˜¾å¢åŠ ï¼Œä½¿ç”¨å•ä¸€æœ€ä½³æ•°æ®æº")
            
            time_span_days = (best_data.index[-1] - best_data.index[0]).total_seconds()/86400
            time_span_years = time_span_days / 365
            
            print(f"â° æ—¶é—´è·¨åº¦: {time_span_days:.1f}å¤© ({time_span_years:.2f}å¹´)")
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {best_data.index[0]} åˆ° {best_data.index[-1]}")
            print(f"ğŸ’° ä»·æ ¼èŒƒå›´: {best_data['close'].min():.6f} - {best_data['close'].max():.6f}")
            
            return best_source, best_data
        else:
            print("âŒ æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥äº†")
            return None, None
    
    def _fetch_yahoo_finance(self):
        """è·å– Yahoo Finance çš„DOGEæ•°æ®ï¼ˆæ··åˆç­–ç•¥ï¼š15åˆ†é’Ÿ+1å°æ—¶ï¼‰"""
        try:
            print("   ğŸ”„ å°è¯• Yahoo Finance (æ··åˆç­–ç•¥)...")
            
            # ä½¿ç”¨å·²éªŒè¯çš„DOGEç¬¦å·
            symbol = "DOGE-USD"
            ticker = yf.Ticker(symbol)
            
            print(f"   ğŸ“¥ è·å–ç¬¦å·: {symbol}")
            
            # ç­–ç•¥1: è·å–è¿‘æœŸ60å¤©çš„15åˆ†é’Ÿé«˜ç²¾åº¦æ•°æ®
            print("   ğŸ“Š ç­–ç•¥1: è·å–è¿‘æœŸ60å¤©15åˆ†é’Ÿæ•°æ®...")
            try:
                data_15m = ticker.history(period="60d", interval="15m", auto_adjust=False, prepost=False)
                if len(data_15m) > 100:
                    days_15m = (data_15m.index[-1] - data_15m.index[0]).total_seconds() / 86400
                    print(f"   âœ… 15åˆ†é’Ÿæ•°æ®: {len(data_15m)}æ¡, {days_15m:.1f}å¤©")
                    
                    # è½¬æ¢æ ¼å¼
                    df_15m = pd.DataFrame()
                    df_15m['open'] = data_15m['Open']
                    df_15m['high'] = data_15m['High'] 
                    df_15m['low'] = data_15m['Low']
                    df_15m['close'] = data_15m['Close']
                    df_15m['volume'] = data_15m['Volume']
                    
                    # ç»Ÿä¸€æ—¶åŒºå¤„ç†
                    if data_15m.index.tz is not None:
                        df_15m.index = data_15m.index.tz_convert('UTC').tz_localize(None)
                    else:
                        df_15m.index = pd.to_datetime(data_15m.index)
                    
                    # ç­–ç•¥2: è·å–1å¹´çš„1å°æ—¶æ•°æ®ä½œä¸ºè¡¥å……
                    print("   ğŸ“Š ç­–ç•¥2: è·å–1å¹´1å°æ—¶æ•°æ®ä½œä¸ºå†å²è¡¥å……...")
                    try:
                        data_1h = ticker.history(period="1y", interval="1h", auto_adjust=False, prepost=False)
                        if len(data_1h) > 100:
                            days_1h = (data_1h.index[-1] - data_1h.index[0]).total_seconds() / 86400
                            print(f"   âœ… 1å°æ—¶æ•°æ®: {len(data_1h)}æ¡, {days_1h:.1f}å¤©")
                            
                            # è½¬æ¢æ ¼å¼
                            df_1h = pd.DataFrame()
                            df_1h['open'] = data_1h['Open']
                            df_1h['high'] = data_1h['High'] 
                            df_1h['low'] = data_1h['Low']
                            df_1h['close'] = data_1h['Close']
                            df_1h['volume'] = data_1h['Volume']
                            
                            # ç»Ÿä¸€æ—¶åŒºå¤„ç†
                            if data_1h.index.tz is not None:
                                df_1h.index = data_1h.index.tz_convert('UTC').tz_localize(None)
                            else:
                                df_1h.index = pd.to_datetime(data_1h.index)
                            
                            # ç­–ç•¥3: æ™ºèƒ½åˆå¹¶æ•°æ®
                            print("   ğŸ”„ æ™ºèƒ½åˆå¹¶é«˜ç²¾åº¦å’Œé•¿æœŸæ•°æ®...")
                            
                            # æ‰¾åˆ°15åˆ†é’Ÿæ•°æ®çš„å¼€å§‹æ—¶é—´
                            cutoff_time = df_15m.index[0]
                            
                            # åªä¿ç•™15åˆ†é’Ÿæ•°æ®å¼€å§‹æ—¶é—´ä¹‹å‰çš„1å°æ—¶æ•°æ®
                            df_1h_historical = df_1h[df_1h.index < cutoff_time]
                            
                            if len(df_1h_historical) > 0:
                                # åˆå¹¶æ•°æ®ï¼šå†å²1å°æ—¶æ•°æ® + è¿‘æœŸ15åˆ†é’Ÿæ•°æ®
                                combined_df = pd.concat([df_1h_historical, df_15m]).sort_index()
                                combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
                                
                                total_days = (combined_df.index[-1] - combined_df.index[0]).total_seconds() / 86400
                                historical_days = (df_1h_historical.index[-1] - df_1h_historical.index[0]).total_seconds() / 86400
                                
                                print(f"   ğŸ‰ æ··åˆæ•°æ®åˆå¹¶æˆåŠŸ!")
                                print(f"      ğŸ“ˆ æ€»æ•°æ®é‡: {len(combined_df)}æ¡")
                                print(f"      â° æ€»æ—¶é—´è·¨åº¦: {total_days:.1f}å¤© ({total_days/365:.2f}å¹´)")
                                print(f"      ğŸ“Š å†å²éƒ¨åˆ†: {len(df_1h_historical)}æ¡1å°æ—¶æ•°æ® ({historical_days:.1f}å¤©)")
                                print(f"      ğŸ” è¿‘æœŸéƒ¨åˆ†: {len(df_15m)}æ¡15åˆ†é’Ÿæ•°æ® ({days_15m:.1f}å¤©)")
                                
                                return combined_df
                            else:
                                print("   âš ï¸ æ— å†å²æ•°æ®å¯åˆå¹¶ï¼Œè¿”å›15åˆ†é’Ÿæ•°æ®")
                                return df_15m
                        else:
                            print("   âš ï¸ 1å°æ—¶æ•°æ®è·å–å¤±è´¥ï¼Œè¿”å›15åˆ†é’Ÿæ•°æ®")
                            return df_15m
                    except Exception as e:
                        print(f"   âš ï¸ 1å°æ—¶æ•°æ®è·å–å¤±è´¥: {str(e)}ï¼Œè¿”å›15åˆ†é’Ÿæ•°æ®")
                        return df_15m
                else:
                    print("   âŒ 15åˆ†é’Ÿæ•°æ®è·å–å¤±è´¥")
            except Exception as e:
                print(f"   âŒ 15åˆ†é’Ÿæ•°æ®è·å–å¤±è´¥: {str(e)}")
            
            # å¤‡ç”¨ç­–ç•¥: å¦‚æœ15åˆ†é’Ÿæ•°æ®å¤±è´¥ï¼Œå°è¯•è·å–1å°æ—¶æ•°æ®
            print("   ğŸ“Š å¤‡ç”¨ç­–ç•¥: å°è¯•1å°æ—¶æ•°æ®...")
            try:
                data_1h = ticker.history(period="1y", interval="1h", auto_adjust=False, prepost=False)
                if len(data_1h) > 100:
                    days = (data_1h.index[-1] - data_1h.index[0]).total_seconds() / 86400
                    print(f"   âœ… å¤‡ç”¨1å°æ—¶æ•°æ®: {len(data_1h)}æ¡, {days:.1f}å¤©")
                    
                    # è½¬æ¢æ ¼å¼
                    df = pd.DataFrame()
                    df['open'] = data_1h['Open']
                    df['high'] = data_1h['High'] 
                    df['low'] = data_1h['Low']
                    df['close'] = data_1h['Close']
                    df['volume'] = data_1h['Volume']
                    
                    # ç»Ÿä¸€æ—¶åŒºå¤„ç†
                    if data_1h.index.tz is not None:
                        df.index = data_1h.index.tz_convert('UTC').tz_localize(None)
                    else:
                        df.index = pd.to_datetime(data_1h.index)
                    
                    return df
            except Exception as e:
                print(f"   âŒ å¤‡ç”¨1å°æ—¶æ•°æ®å¤±è´¥: {str(e)}")
            
            print(f"   âŒ Yahoo Finance æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥")
            return None
            
        except Exception as e:
            print(f"   âŒ Yahoo Finance æ•´ä½“å¤±è´¥: {str(e)}")
            return None
    
    def _fetch_kraken_ohlc(self):
        """è·å– Kraken OHLC æ•°æ®"""
        try:
            print("   ğŸ”„ å°è¯• Kraken API...")
            
            # å°è¯•å¤šä¸ªKrakençš„DOGEäº¤æ˜“å¯¹
            pairs = ["DOGEUSD", "XDGUSD"]
            
            for pair in pairs:
                try:
                    print(f"   ğŸ“¡ å°è¯• Kraken {pair}...")
                    
                    url = "https://api.kraken.com/0/public/OHLC"
                    params = {
                        "pair": pair,
                        "interval": 15,  # 15åˆ†é’Ÿé—´éš”
                        "since": int((datetime.now() - timedelta(days=30)).timestamp())
                    }
                    
                    response = requests.get(url, params=params, timeout=30)
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        if data.get("error") == [] and data.get("result"):
                            # Kraken è¿”å›çš„keyå¯èƒ½ä¸åŒ
                            result_key = None
                            for key in data["result"].keys():
                                if key != "last":
                                    result_key = key
                                    break
                            
                            if result_key and len(data["result"][result_key]) > 10:
                                df = self._convert_kraken_format(data["result"][result_key])
                                if df is not None:
                                    print(f"   âœ… Kraken {pair} æˆåŠŸ: {len(df)}æ¡æ•°æ®")
                                    return df
                        else:
                            print(f"   âš ï¸ Kraken {pair} APIé”™è¯¯: {data.get('error', 'Unknown')}")
                    else:
                        print(f"   âš ï¸ Kraken {pair} HTTPé”™è¯¯: {response.status_code}")
                    
                except Exception as e:
                    print(f"   âš ï¸ Kraken {pair} å¤±è´¥: {str(e)}")
                    continue
            
            print(f"   âŒ Kraken æ‰€æœ‰äº¤æ˜“å¯¹éƒ½å¤±è´¥")
            return None
            
        except Exception as e:
            print(f"   âŒ Kraken æ•´ä½“å¤±è´¥: {str(e)}")
            return None

    def _convert_kraken_format(self, data):
        """è½¬æ¢ Kraken OHLC æ•°æ®æ ¼å¼"""
        try:
            # Kraken OHLCæ ¼å¼: [timestamp, open, high, low, close, vwap, volume, count]
            df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'vwap', 'volume', 'count'])
            
            # è½¬æ¢æ•°æ®ç±»å‹
            for col in ['open', 'high', 'low', 'close', 'volume']:
                df[col] = pd.to_numeric(df[col])
            
            # ç»Ÿä¸€æ—¶åŒºå¤„ç†
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
            df.set_index('timestamp', inplace=True)
            
            return df[['open', 'high', 'low', 'close', 'volume']].sort_index()
            
        except Exception as e:
            print(f"   âŒ Kraken æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
            return None

    def _fetch_huobi_15min(self):
        """è·å–Huobi 15åˆ†é’Ÿæ•°æ®"""
        try:
            symbol = self.symbol.replace("USDT", "usdt").lower()
            
            print("   ğŸ”„ è·å–Huobiæ•°æ®(æœ€ä¼˜15åˆ†é’Ÿ)...")
            
            # ç›´æ¥è·å–15åˆ†é’Ÿé«˜è´¨é‡æ•°æ®
            url = "https://api.huobi.pro/market/history/kline"
            params = {
                "symbol": symbol,
                "period": "15min",
                "size": 2000  # è·å–æœ€å¤§å¯èƒ½çš„æ•°æ®é‡
            }
            
            response = requests.get(url, params=params, timeout=15)
            response.raise_for_status()
            result = response.json()
            
            if result.get("status") == "ok" and result.get("data"):
                data = result["data"]
                
                if len(data) > 100:
                    df = self._convert_huobi_data(data)
                    
                    # æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
                    min_ts = min(item['id'] for item in data)
                    max_ts = max(item['id'] for item in data)
                    min_time = datetime.fromtimestamp(min_ts)
                    max_time = datetime.fromtimestamp(max_ts)
                    time_span = (max_ts - min_ts) / (24 * 3600)  # å¤©æ•°
                    
                    print(f"   âœ… Huobi 15åˆ†é’Ÿæ•°æ®æˆåŠŸ: {len(df)}æ¡")
                    print(f"      ğŸ“… æ—¶é—´èŒƒå›´: {min_time} åˆ° {max_time}")
                    print(f"      â° æ—¶é—´è·¨åº¦: {time_span:.1f}å¤©")
                    print(f"      ğŸ’° ä»·æ ¼èŒƒå›´: {df['close'].min():.6f} - {df['close'].max():.6f} USDT")
                    print(f"      ğŸ“Š æ•°æ®å¯†åº¦: {len(df)/time_span:.1f} æ¡/å¤©")
                    
                    return df
                else:
                    print(f"   âŒ Huobiæ•°æ®é‡ä¸è¶³: {len(data)}æ¡")
            else:
                print(f"   âŒ Huobi APIè¿”å›é”™è¯¯: {result.get('err-msg', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as e:
            print(f"   âŒ Huobiæ•°æ®è·å–å¤±è´¥: {str(e)}")
        
        return None
    
    def _convert_huobi_data(self, data):
        """è½¬æ¢Huobiæ•°æ®æ ¼å¼"""
        df = pd.DataFrame(data)
        
        # Huobiæ•°æ®æ ¼å¼: {id, open, close, low, high, amount, vol, count}
        df = df.rename(columns={
            'id': 'timestamp',
            'vol': 'volume'
        })
        
        for col in ['open', 'high', 'low', 'close', 'volume']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col])
        
        # ç»Ÿä¸€æ—¶åŒºå¤„ç† - å»é™¤æ—¶åŒºä¿¡æ¯
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
        df.set_index('timestamp', inplace=True)
        
        return df[['open', 'high', 'low', 'close', 'volume']].sort_index()
    
    def _merge_multiple_sources_fixed(self, all_data):
        """åˆå¹¶å¤šä¸ªæ•°æ®æºçš„æ•°æ®ï¼ˆä¿®å¤æ—¶åŒºé—®é¢˜ï¼‰"""
        try:
            print("   ğŸ”„ åˆå¹¶æ•°æ®æº...")
            
            # æŒ‰æ•°æ®é‡æ’åºï¼Œæœ€å¤§çš„åœ¨å‰
            sorted_data = sorted(all_data, key=lambda x: len(x[1]), reverse=True)
            
            # ä»æœ€å¤§çš„æ•°æ®æºå¼€å§‹
            combined_df = sorted_data[0][1].copy()
            base_source = sorted_data[0][0]
            
            # ç¡®ä¿åŸºç¡€æ•°æ®æ²¡æœ‰æ—¶åŒºä¿¡æ¯
            if combined_df.index.tz is not None:
                combined_df.index = combined_df.index.tz_localize(None)
            
            print(f"   ğŸ“Š åŸºç¡€æ•°æ®æº: {base_source} ({len(combined_df)}æ¡)")
            
            # åˆå¹¶å…¶ä»–æ•°æ®æº
            for source_name, df in sorted_data[1:]:
                print(f"   ğŸ”„ åˆå¹¶ {source_name} ({len(df)}æ¡)...")
                
                # ç¡®ä¿è¦åˆå¹¶çš„æ•°æ®ä¹Ÿæ²¡æœ‰æ—¶åŒºä¿¡æ¯
                df_to_merge = df.copy()
                if df_to_merge.index.tz is not None:
                    df_to_merge.index = df_to_merge.index.tz_localize(None)
                
                # å»é‡åˆå¹¶ï¼šåªæ·»åŠ æ—¶é—´æˆ³ä¸é‡å¤çš„æ•°æ®
                before_merge = len(combined_df)
                
                # æ‰¾åˆ°ä¸é‡å¤çš„æ—¶é—´æˆ³
                new_timestamps = df_to_merge.index.difference(combined_df.index)
                
                if len(new_timestamps) > 0:
                    new_data = df_to_merge.loc[new_timestamps]
                    combined_df = pd.concat([combined_df, new_data]).sort_index()
                    
                    print(f"   âœ… æ·»åŠ äº† {len(new_data)} æ¡æ–°æ•°æ®")
                else:
                    print(f"   âš ï¸ æ— æ–°æ•°æ®å¯æ·»åŠ ")
            
            print(f"   ğŸ‰ åˆå¹¶å®Œæˆ: æ€»è®¡ {len(combined_df)} æ¡æ•°æ®")
            
            return combined_df
            
        except Exception as e:
            print(f"   âŒ æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
            return None

def test_enhanced_fetcher():
    """æµ‹è¯•ä¼˜åŒ–ç‰ˆæ•°æ®è·å–å™¨"""
    print("ğŸ§ª æµ‹è¯•ä¼˜åŒ–ç‰ˆå¤šæ•°æ®æºè·å–å™¨")
    print("=" * 60)
    
    fetcher = EnhancedMultiSourceDataFetcher("DOGEUSDT")
    source, data = fetcher.fetch_all_sources(target_points=2000)
    
    if data is not None:
        print(f"\nğŸ‰ å¤šæ•°æ®æºè·å–æˆåŠŸï¼")
        print("=" * 60)
        
        time_span_days = (data.index[-1] - data.index[0]).total_seconds()/86400
        time_span_years = time_span_days / 365
        
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®è·å–ç»“æœ:")
        print(f"   ğŸ† æœ€ä½³æ•°æ®æº: {source}")
        print(f"   ğŸ“ˆ æ•°æ®é‡: {len(data):,}æ¡")
        print(f"   â° æ—¶é—´è·¨åº¦: {time_span_days:.1f}å¤© ({time_span_years:.2f}å¹´)")
        print(f"   ğŸ“… æ—¶é—´èŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")
        print(f"   ğŸ’° ä»·æ ¼èŒƒå›´: {data['close'].min():.6f} - {data['close'].max():.6f}")
        print(f"   ğŸ“Š å¹³å‡æ—¥å¯†åº¦: {len(data)/time_span_days:.1f} æ¡/å¤©")
        
        # æ•°æ®è´¨é‡è¯„ä¼°
        if len(data) >= 4000:
            print(f"   ğŸ† æ•°æ®è´¨é‡: å“è¶Š (4,000+æ¡æ•°æ®)")
        elif len(data) >= 2000:
            print(f"   ğŸ¥‡ æ•°æ®è´¨é‡: ä¼˜ç§€ (2,000+æ¡æ•°æ®)")
        elif len(data) >= 1000:
            print(f"   ğŸ‘ æ•°æ®è´¨é‡: è‰¯å¥½ (1,000+æ¡æ•°æ®)")
        else:
            print(f"   âš ï¸ æ•°æ®è´¨é‡: ä¸€èˆ¬ (å°‘äº1,000æ¡)")
            
        # æ•°æ®æ—¶é—´è·¨åº¦è¯„ä¼°
        if time_span_years >= 0.8:
            print(f"   ğŸ“… æ—¶é—´è·¨åº¦: ä¼˜ç§€ (æ¥è¿‘1å¹´æ•°æ®)")
        elif time_span_years >= 0.5:
            print(f"   ğŸ“… æ—¶é—´è·¨åº¦: è‰¯å¥½ (åŠå¹´ä»¥ä¸Šæ•°æ®)")
        else:
            print(f"   ğŸ“… æ—¶é—´è·¨åº¦: ä¸€èˆ¬ (å°‘äºåŠå¹´æ•°æ®)")
        
        return True
    else:
        print("âŒ æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥äº†")
        return False

# ä¿æŒå‘åå…¼å®¹æ€§
class MultiSourceDataFetcher(EnhancedMultiSourceDataFetcher):
    """å‘åå…¼å®¹çš„æ•°æ®è·å–å™¨"""
    
    def fetch_large_dataset(self, target_points=2000):
        """ä¿æŒå‘åå…¼å®¹çš„æ–¹æ³•ï¼Œç›´æ¥è°ƒç”¨æ–°çš„å¤šæºè·å–"""
        return self.fetch_all_sources(target_points)

def test_huobi_fetcher():
    """ä¿æŒå‘åå…¼å®¹çš„æµ‹è¯•å‡½æ•°"""
    return test_enhanced_fetcher()

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•ä¼˜åŒ–åçš„æ•°æ®æº...")
    print("ğŸ’¡ åªä¿ç•™çœŸæ­£æœ‰æ•ˆçš„æ•°æ®æº")
    print("ğŸ“Š ç›®æ ‡ï¼šè·å–DOGE 15åˆ†é’Ÿæ•°æ®ï¼Œæœ€å¥½ä¸€å¹´å†å²")
    print()
    
    test_enhanced_fetcher() 